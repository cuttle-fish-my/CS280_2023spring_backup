\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{PyTorch}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \hypertarget{whats-this-pytorch-business}{%
\section{What's this PyTorch
business?}\label{whats-this-pytorch-business}}

You've written a lot of code in this assignment to provide a whole host
of neural network functionality. Dropout, Batch Norm, and 2D
convolutions are some of the workhorses of deep learning in computer
vision. You've also worked hard to make your code efficient and
vectorized.

For the last part of this assignment, though, we're going to leave
behind your beautiful codebase and instead migrate to one of two popular
deep learning frameworks: in this instance, PyTorch (or TensorFlow, if
you choose to use that notebook).

    \hypertarget{what-is-pytorch}{%
\subsubsection{What is PyTorch?}\label{what-is-pytorch}}

PyTorch is a system for executing dynamic computational graphs over
Tensor objects that behave similarly as numpy ndarray. It comes with a
powerful automatic differentiation engine that removes the need for
manual back-propagation.

\hypertarget{why}{%
\subsubsection{Why?}\label{why}}

\begin{itemize}
\tightlist
\item
  Our code will now run on GPUs! Much faster training. When using a
  framework like PyTorch or TensorFlow you can harness the power of the
  GPU for your own custom neural network architectures without having to
  write CUDA code directly (which is beyond the scope of this class).
\item
  We want you to be ready to use one of these frameworks for your
  project so you can experiment more efficiently than if you were
  writing every feature you want to use by hand.
\item
  We want you to stand on the shoulders of giants! TensorFlow and
  PyTorch are both excellent frameworks that will make your lives a lot
  easier, and now that you understand their guts, you are free to use
  them :)
\item
  We want you to be exposed to the sort of deep learning code you might
  run into in academia or industry.
\end{itemize}

\hypertarget{pytorch-versions}{%
\subsubsection{PyTorch versions}\label{pytorch-versions}}

This notebook assumes that you are using \textbf{PyTorch version 1.4}.
In some of the previous versions (e.g.~before 0.4), Tensors had to be
wrapped in Variable objects to be used in autograd; however Variables
have now been deprecated. In addition 1.0+ versions separate a Tensor's
datatype from its device, and use numpy-style factories for constructing
Tensors rather than directly invoking Tensor constructors.

    \hypertarget{how-will-i-learn-pytorch}{%
\subsection{How will I learn PyTorch?}\label{how-will-i-learn-pytorch}}

Justin Johnson has made an excellent
\href{https://github.com/jcjohnson/pytorch-examples}{tutorial} for
PyTorch.

You can also find the detailed
\href{http://pytorch.org/docs/stable/index.html}{API doc} here. If you
have other questions that are not addressed by the API docs, the
\href{https://discuss.pytorch.org/}{PyTorch forum} is a much better
place to ask than StackOverflow.

\hypertarget{install-pytorch-1.4-only-if-you-are-working-locally}{%
\subsection{Install PyTorch 1.4 (ONLY IF YOU ARE WORKING
LOCALLY)}\label{install-pytorch-1.4-only-if-you-are-working-locally}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Have the latest version of Anaconda installed on your machine.
\item
  Create a new conda environment starting from Python 3.7. In this setup
  example, we'll call it \texttt{torch\_env}.
\item
  Run the command: \texttt{conda\ activate\ torch\_env}
\item
  Run the command: \texttt{pip\ install\ torch==1.4\ torchvision==0.5.0}
\end{enumerate}

    \hypertarget{table-of-contents}{%
\section{Table of Contents}\label{table-of-contents}}

This assignment has 5 parts. You will learn PyTorch on \textbf{three
different levels of abstraction}, which will help you understand it
better and prepare you for the final project.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Part I, Preparation: we will use CIFAR-10 dataset.
\item
  Part II, Barebones PyTorch: \textbf{Abstraction level 1}, we will work
  directly with the lowest-level PyTorch Tensors.
\item
  Part III, PyTorch Module API: \textbf{Abstraction level 2}, we will
  use \texttt{nn.Module} to define arbitrary neural network
  architecture.
\item
  Part IV, PyTorch Sequential API: \textbf{Abstraction level 3}, we will
  use \texttt{nn.Sequential} to define a linear feed-forward network
  very conveniently.
\item
  Part V, CIFAR-10 open-ended challenge: please implement your own
  network to get as high accuracy as possible on CIFAR-10. You can
  experiment with any layer, optimizer, hyperparameters or other
  advanced features.
\end{enumerate}

Here is a table of comparison:

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
API & Flexibility & Convenience \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Barebone & High & Low \\
\texttt{nn.Module} & High & Medium \\
\texttt{nn.Sequential} & Low & High \\
\end{longtable}

    \hypertarget{part-i.-preparation}{%
\section{Part I. Preparation}\label{part-i.-preparation}}

First, we load the CIFAR-10 dataset. This might take a couple minutes
the first time you do it, but the files should stay cached after that.

In previous parts of the assignment we had to write our own code to
download the CIFAR-10 dataset, preprocess it, and iterate through it in
minibatches; PyTorch provides convenient tools to automate this process
for us.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{torch}
\PY{c+c1}{\PYZsh{} assert \PYZsq{}.\PYZsq{}.join(torch.\PYZus{}\PYZus{}version\PYZus{}\PYZus{}.split(\PYZsq{}.\PYZsq{})[:2]) == \PYZsq{}1.4\PYZsq{} \PYZsh{} sorry for commenting this line since I can only use torch \PYZgt{}= 1.9.0 with Apple Silicon chip}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn} \PY{k}{as} \PY{n+nn}{nn}
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{optim} \PY{k}{as} \PY{n+nn}{optim}
\PY{k+kn}{from} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{utils}\PY{n+nn}{.}\PY{n+nn}{data} \PY{k+kn}{import} \PY{n}{DataLoader}
\PY{k+kn}{from} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{utils}\PY{n+nn}{.}\PY{n+nn}{data} \PY{k+kn}{import} \PY{n}{sampler}

\PY{k+kn}{import} \PY{n+nn}{torchvision}\PY{n+nn}{.}\PY{n+nn}{datasets} \PY{k}{as} \PY{n+nn}{dset}
\PY{k+kn}{import} \PY{n+nn}{torchvision}\PY{n+nn}{.}\PY{n+nn}{transforms} \PY{k}{as} \PY{n+nn}{T}

\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{NUM\PYZus{}TRAIN} \PY{o}{=} \PY{l+m+mi}{49000}

\PY{c+c1}{\PYZsh{} The torchvision.transforms package provides tools for preprocessing data}
\PY{c+c1}{\PYZsh{} and for performing data augmentation; here we set up a transform to}
\PY{c+c1}{\PYZsh{} preprocess the data by subtracting the mean RGB value and dividing by the}
\PY{c+c1}{\PYZsh{} standard deviation of each RGB value; we\PYZsq{}ve hardcoded the mean and std.}
\PY{n}{transform} \PY{o}{=} \PY{n}{T}\PY{o}{.}\PY{n}{Compose}\PY{p}{(}\PY{p}{[}
                \PY{n}{T}\PY{o}{.}\PY{n}{ToTensor}\PY{p}{(}\PY{p}{)}\PY{p}{,}
                \PY{n}{T}\PY{o}{.}\PY{n}{Normalize}\PY{p}{(}\PY{p}{(}\PY{l+m+mf}{0.4914}\PY{p}{,} \PY{l+m+mf}{0.4822}\PY{p}{,} \PY{l+m+mf}{0.4465}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+m+mf}{0.2023}\PY{p}{,} \PY{l+m+mf}{0.1994}\PY{p}{,} \PY{l+m+mf}{0.2010}\PY{p}{)}\PY{p}{)}
            \PY{p}{]}\PY{p}{)}

\PY{c+c1}{\PYZsh{} We set up a Dataset object for each split (train / val / test); Datasets load}
\PY{c+c1}{\PYZsh{} training examples one at a time, so we wrap each Dataset in a DataLoader which}
\PY{c+c1}{\PYZsh{} iterates through the Dataset and forms minibatches. We divide the CIFAR\PYZhy{}10}
\PY{c+c1}{\PYZsh{} training set into train and val sets by passing a Sampler object to the}
\PY{c+c1}{\PYZsh{} DataLoader telling how it should sample from the underlying Dataset.}
\PY{n}{cifar10\PYZus{}train} \PY{o}{=} \PY{n}{dset}\PY{o}{.}\PY{n}{CIFAR10}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./cs231n/datasets}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{train}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{download}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
                             \PY{n}{transform}\PY{o}{=}\PY{n}{transform}\PY{p}{)}
\PY{n}{loader\PYZus{}train} \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}\PY{n}{cifar10\PYZus{}train}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} 
                          \PY{n}{sampler}\PY{o}{=}\PY{n}{sampler}\PY{o}{.}\PY{n}{SubsetRandomSampler}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n}{NUM\PYZus{}TRAIN}\PY{p}{)}\PY{p}{)}\PY{p}{)}

\PY{n}{cifar10\PYZus{}val} \PY{o}{=} \PY{n}{dset}\PY{o}{.}\PY{n}{CIFAR10}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./cs231n/datasets}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{train}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} \PY{n}{download}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}
                           \PY{n}{transform}\PY{o}{=}\PY{n}{transform}\PY{p}{)}
\PY{n}{loader\PYZus{}val} \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}\PY{n}{cifar10\PYZus{}val}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{,} 
                        \PY{n}{sampler}\PY{o}{=}\PY{n}{sampler}\PY{o}{.}\PY{n}{SubsetRandomSampler}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n}{NUM\PYZus{}TRAIN}\PY{p}{,} \PY{l+m+mi}{50000}\PY{p}{)}\PY{p}{)}\PY{p}{)}

\PY{n}{cifar10\PYZus{}test} \PY{o}{=} \PY{n}{dset}\PY{o}{.}\PY{n}{CIFAR10}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{./cs231n/datasets}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{train}\PY{o}{=}\PY{k+kc}{False}\PY{p}{,} \PY{n}{download}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,} 
                            \PY{n}{transform}\PY{o}{=}\PY{n}{transform}\PY{p}{)}
\PY{n}{loader\PYZus{}test} \PY{o}{=} \PY{n}{DataLoader}\PY{p}{(}\PY{n}{cifar10\PYZus{}test}\PY{p}{,} \PY{n}{batch\PYZus{}size}\PY{o}{=}\PY{l+m+mi}{64}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
    \end{Verbatim}

    You have an option to \textbf{use GPU by setting the flag to True
below}. It is not necessary to use GPU for this assignment. Note that if
your computer does not have CUDA enabled,
\texttt{torch.cuda.is\_available()} will return False and this notebook
will fallback to CPU mode.

The global variables \texttt{dtype} and \texttt{device} will control the
data types throughout this assignment.

\hypertarget{colab-users}{%
\subsection{Colab Users}\label{colab-users}}

If you are using Colab, you need to manually switch to a GPU device. You
can do this by clicking
\texttt{Runtime\ -\textgreater{}\ Change\ runtime\ type} and selecting
\texttt{GPU} under \texttt{Hardware\ Accelerator}. Note that you have to
rerun the cells from the top since the kernel gets restarted upon
switching runtimes.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{USE\PYZus{}GPU} \PY{o}{=} \PY{k+kc}{True}

\PY{n}{dtype} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{float32} \PY{c+c1}{\PYZsh{} we will be using float throughout this tutorial}

\PY{k}{if} \PY{n}{USE\PYZus{}GPU} \PY{o+ow}{and} \PY{n}{torch}\PY{o}{.}\PY{n}{cuda}\PY{o}{.}\PY{n}{is\PYZus{}available}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{device} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{device}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cuda}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{k}{else}\PY{p}{:}
    \PY{k+kn}{import} \PY{n+nn}{platform}
    \PY{n}{device} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{device}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cpu}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{k}{if} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{macOS}\PY{l+s+s2}{\PYZdq{}} \PY{o+ow}{in} \PY{n}{platform}\PY{o}{.}\PY{n}{platform}\PY{p}{(}\PY{p}{)} \PY{o+ow}{and} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{arm\PYZhy{}64bit}\PY{l+s+s2}{\PYZdq{}} \PY{o+ow}{in} \PY{n}{platform}\PY{o}{.}\PY{n}{platform}\PY{p}{(}\PY{p}{)}\PY{p}{:}
        \PY{n}{device} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{device}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{mps}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Constant to control how frequently we print train loss}
\PY{n}{print\PYZus{}every} \PY{o}{=} \PY{l+m+mi}{100}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{using device:}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{device}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
using device: mps
    \end{Verbatim}

    \hypertarget{part-ii.-barebones-pytorch}{%
\section{Part II. Barebones PyTorch}\label{part-ii.-barebones-pytorch}}

PyTorch ships with high-level APIs to help us define model architectures
conveniently, which we will cover in Part II of this tutorial. In this
section, we will start with the barebone PyTorch elements to understand
the autograd engine better. After this exercise, you will come to
appreciate the high-level model API more.

We will start with a simple fully-connected ReLU network with two hidden
layers and no biases for CIFAR classification. This implementation
computes the forward pass using operations on PyTorch Tensors, and uses
PyTorch autograd to compute gradients. It is important that you
understand every line, because you will write a harder version after the
example.

When we create a PyTorch Tensor with \texttt{requires\_grad=True}, then
operations involving that Tensor will not just compute values; they will
also build up a computational graph in the background, allowing us to
easily backpropagate through the graph to compute gradients of some
Tensors with respect to a downstream loss. Concretely if x is a Tensor
with \texttt{x.requires\_grad\ ==\ True} then after backpropagation
\texttt{x.grad} will be another Tensor holding the gradient of x with
respect to the scalar loss at the end.

    \hypertarget{pytorch-tensors-flatten-function}{%
\subsubsection{PyTorch Tensors: Flatten
Function}\label{pytorch-tensors-flatten-function}}

A PyTorch Tensor is conceptionally similar to a numpy array: it is an
n-dimensional grid of numbers, and like numpy PyTorch provides many
functions to efficiently operate on Tensors. As a simple example, we
provide a \texttt{flatten} function below which reshapes image data for
use in a fully-connected neural network.

Recall that image data is typically stored in a Tensor of shape N x C x
H x W, where:

\begin{itemize}
\tightlist
\item
  N is the number of datapoints
\item
  C is the number of channels
\item
  H is the height of the intermediate feature map in pixels
\item
  W is the height of the intermediate feature map in pixels
\end{itemize}

This is the right way to represent the data when we are doing something
like a 2D convolution, that needs spatial understanding of where the
intermediate features are relative to each other. When we use fully
connected affine layers to process the image, however, we want each
datapoint to be represented by a single vector -- it's no longer useful
to segregate the different channels, rows, and columns of the data. So,
we use a ``flatten'' operation to collapse the \texttt{C\ x\ H\ x\ W}
values per representation into a single long vector. The flatten
function below first reads in the N, C, H, and W values from a given
batch of data, and then returns a ``view'' of that data. ``View'' is
analogous to numpy's ``reshape'' method: it reshapes x's dimensions to
be N x ??, where ?? is allowed to be anything (in this case, it will be
C x H x W, but we don't need to specify that explicitly).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{flatten}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
    \PY{n}{N} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{c+c1}{\PYZsh{} read in N, C, H, W}
    \PY{k}{return} \PY{n}{x}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{n}{N}\PY{p}{,} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}  \PY{c+c1}{\PYZsh{} \PYZdq{}flatten\PYZdq{} the C * H * W values into a single vector per image}

\PY{k}{def} \PY{n+nf}{test\PYZus{}flatten}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{x} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{l+m+mi}{12}\PY{p}{)}\PY{o}{.}\PY{n}{view}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Before flattening: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{x}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{After flattening: }\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{flatten}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}

\PY{n}{test\PYZus{}flatten}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Before flattening:  tensor([[[[ 0,  1],
          [ 2,  3],
          [ 4,  5]]],


        [[[ 6,  7],
          [ 8,  9],
          [10, 11]]]])
After flattening:  tensor([[ 0,  1,  2,  3,  4,  5],
        [ 6,  7,  8,  9, 10, 11]])
    \end{Verbatim}

    \hypertarget{barebones-pytorch-two-layer-network}{%
\subsubsection{Barebones PyTorch: Two-Layer
Network}\label{barebones-pytorch-two-layer-network}}

Here we define a function \texttt{two\_layer\_fc} which performs the
forward pass of a two-layer fully-connected ReLU network on a batch of
image data. After defining the forward pass we check that it doesn't
crash and that it produces outputs of the right shape by running zeros
through the network.

You don't have to write any code here, but it's important that you read
and understand the implementation.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{5}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn}\PY{n+nn}{.}\PY{n+nn}{functional} \PY{k}{as} \PY{n+nn}{F}  \PY{c+c1}{\PYZsh{} useful stateless functions}

\PY{k}{def} \PY{n+nf}{two\PYZus{}layer\PYZus{}fc}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{params}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    A fully\PYZhy{}connected neural networks; the architecture is:}
\PY{l+s+sd}{    NN is fully connected \PYZhy{}\PYZgt{} ReLU \PYZhy{}\PYZgt{} fully connected layer.}
\PY{l+s+sd}{    Note that this function only defines the forward pass; }
\PY{l+s+sd}{    PyTorch will take care of the backward pass for us.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    The input to the network will be a minibatch of data, of shape}
\PY{l+s+sd}{    (N, d1, ..., dM) where d1 * ... * dM = D. The hidden layer will have H units,}
\PY{l+s+sd}{    and the output layer will produce scores for C classes.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Inputs:}
\PY{l+s+sd}{    \PYZhy{} x: A PyTorch Tensor of shape (N, d1, ..., dM) giving a minibatch of}
\PY{l+s+sd}{      input data.}
\PY{l+s+sd}{    \PYZhy{} params: A list [w1, w2] of PyTorch Tensors giving weights for the network;}
\PY{l+s+sd}{      w1 has shape (D, H) and w2 has shape (H, C).}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{    \PYZhy{} scores: A PyTorch Tensor of shape (N, C) giving classification scores for}
\PY{l+s+sd}{      the input data x.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{c+c1}{\PYZsh{} first we flatten the image}
    \PY{n}{x} \PY{o}{=} \PY{n}{flatten}\PY{p}{(}\PY{n}{x}\PY{p}{)}  \PY{c+c1}{\PYZsh{} shape: [batch\PYZus{}size, C x H x W]}
    
    \PY{n}{w1}\PY{p}{,} \PY{n}{w2} \PY{o}{=} \PY{n}{params}
    
    \PY{c+c1}{\PYZsh{} Forward pass: compute predicted y using operations on Tensors. Since w1 and}
    \PY{c+c1}{\PYZsh{} w2 have requires\PYZus{}grad=True, operations involving these Tensors will cause}
    \PY{c+c1}{\PYZsh{} PyTorch to build a computational graph, allowing automatic computation of}
    \PY{c+c1}{\PYZsh{} gradients. Since we are no longer implementing the backward pass by hand we}
    \PY{c+c1}{\PYZsh{} don\PYZsq{}t need to keep references to intermediate values.}
    \PY{c+c1}{\PYZsh{} you can also use `.clamp(min=0)`, equivalent to F.relu()}
    \PY{n}{x} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n}{x}\PY{o}{.}\PY{n}{mm}\PY{p}{(}\PY{n}{w1}\PY{p}{)}\PY{p}{)}
    \PY{n}{x} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{mm}\PY{p}{(}\PY{n}{w2}\PY{p}{)}
    \PY{k}{return} \PY{n}{x}
    

\PY{k}{def} \PY{n+nf}{two\PYZus{}layer\PYZus{}fc\PYZus{}test}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{hidden\PYZus{}layer\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{42}
    \PY{n}{x} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{64}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{)}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{dtype}\PY{p}{)}  \PY{c+c1}{\PYZsh{} minibatch size 64, feature dimension 50}
    \PY{n}{w1} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{,} \PY{n}{hidden\PYZus{}layer\PYZus{}size}\PY{p}{)}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{dtype}\PY{p}{)}
    \PY{n}{w2} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n}{hidden\PYZus{}layer\PYZus{}size}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{dtype}\PY{p}{)}
    \PY{n}{scores} \PY{o}{=} \PY{n}{two\PYZus{}layer\PYZus{}fc}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{p}{[}\PY{n}{w1}\PY{p}{,} \PY{n}{w2}\PY{p}{]}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{n}{scores}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{p}{)}\PY{p}{)}  \PY{c+c1}{\PYZsh{} you should see [64, 10]}

\PY{n}{two\PYZus{}layer\PYZus{}fc\PYZus{}test}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([64, 10])
    \end{Verbatim}

    \hypertarget{barebones-pytorch-three-layer-convnet}{%
\subsubsection{Barebones PyTorch: Three-Layer
ConvNet}\label{barebones-pytorch-three-layer-convnet}}

Here you will complete the implementation of the function
\texttt{three\_layer\_convnet}, which will perform the forward pass of a
three-layer convolutional network. Like above, we can immediately test
our implementation by passing zeros through the network. The network
should have the following architecture:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A convolutional layer (with bias) with \texttt{channel\_1} filters,
  each with shape \texttt{KW1\ x\ KH1}, and zero-padding of two
\item
  ReLU nonlinearity
\item
  A convolutional layer (with bias) with \texttt{channel\_2} filters,
  each with shape \texttt{KW2\ x\ KH2}, and zero-padding of one
\item
  ReLU nonlinearity
\item
  Fully-connected layer with bias, producing scores for C classes.
\end{enumerate}

Note that we have \textbf{no softmax activation} here after our
fully-connected layer: this is because PyTorch's cross entropy loss
performs a softmax activation for you, and by bundling that step in
makes computation more efficient.

\textbf{HINT}: For convolutions:
http://pytorch.org/docs/stable/nn.html\#torch.nn.functional.conv2d; pay
attention to the shapes of convolutional filters!

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{6}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{three\PYZus{}layer\PYZus{}convnet}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{params}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Performs the forward pass of a three\PYZhy{}layer convolutional network with the}
\PY{l+s+sd}{    architecture defined above.}

\PY{l+s+sd}{    Inputs:}
\PY{l+s+sd}{    \PYZhy{} x: A PyTorch Tensor of shape (N, 3, H, W) giving a minibatch of images}
\PY{l+s+sd}{    \PYZhy{} params: A list of PyTorch Tensors giving the weights and biases for the}
\PY{l+s+sd}{      network; should contain the following:}
\PY{l+s+sd}{      \PYZhy{} conv\PYZus{}w1: PyTorch Tensor of shape (channel\PYZus{}1, 3, KH1, KW1) giving weights}
\PY{l+s+sd}{        for the first convolutional layer}
\PY{l+s+sd}{      \PYZhy{} conv\PYZus{}b1: PyTorch Tensor of shape (channel\PYZus{}1,) giving biases for the first}
\PY{l+s+sd}{        convolutional layer}
\PY{l+s+sd}{      \PYZhy{} conv\PYZus{}w2: PyTorch Tensor of shape (channel\PYZus{}2, channel\PYZus{}1, KH2, KW2) giving}
\PY{l+s+sd}{        weights for the second convolutional layer}
\PY{l+s+sd}{      \PYZhy{} conv\PYZus{}b2: PyTorch Tensor of shape (channel\PYZus{}2,) giving biases for the second}
\PY{l+s+sd}{        convolutional layer}
\PY{l+s+sd}{      \PYZhy{} fc\PYZus{}w: PyTorch Tensor giving weights for the fully\PYZhy{}connected layer. Can you}
\PY{l+s+sd}{        figure out what the shape should be?}
\PY{l+s+sd}{      \PYZhy{} fc\PYZus{}b: PyTorch Tensor giving biases for the fully\PYZhy{}connected layer. Can you}
\PY{l+s+sd}{        figure out what the shape should be?}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns:}
\PY{l+s+sd}{    \PYZhy{} scores: PyTorch Tensor of shape (N, C) giving classification scores for x}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{conv\PYZus{}w1}\PY{p}{,} \PY{n}{conv\PYZus{}b1}\PY{p}{,} \PY{n}{conv\PYZus{}w2}\PY{p}{,} \PY{n}{conv\PYZus{}b2}\PY{p}{,} \PY{n}{fc\PYZus{}w}\PY{p}{,} \PY{n}{fc\PYZus{}b} \PY{o}{=} \PY{n}{params}
    \PY{n}{scores} \PY{o}{=} \PY{k+kc}{None}
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
    \PY{c+c1}{\PYZsh{} TODO: Implement the forward pass for the three\PYZhy{}layer ConvNet.                \PYZsh{}}
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
    \PY{c+c1}{\PYZsh{} *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****}

    \PY{n}{conv1\PYZus{}pad} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{conv\PYZus{}w1}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{:}\PY{p}{]}\PY{p}{)} \PY{o}{/}\PY{o}{/} \PY{l+m+mi}{2}
    \PY{n}{conv2\PYZus{}pad} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{conv\PYZus{}w2}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{:}\PY{p}{]}\PY{p}{)} \PY{o}{/}\PY{o}{/} \PY{l+m+mi}{2}
    \PY{n}{scores} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{conv2d}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{conv\PYZus{}w1}\PY{p}{,} \PY{n}{conv\PYZus{}b1}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{conv1\PYZus{}pad}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}\PY{p}{)}
    \PY{n}{scores} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n}{scores}\PY{p}{)}
    \PY{n}{scores} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{conv2d}\PY{p}{(}\PY{n}{scores}\PY{p}{,} \PY{n}{conv\PYZus{}w2}\PY{p}{,} \PY{n}{conv\PYZus{}b2}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{n}{conv2\PYZus{}pad}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}\PY{p}{)}
    \PY{n}{scores} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n}{scores}\PY{p}{)}
    \PY{n}{scores} \PY{o}{=} \PY{n}{flatten}\PY{p}{(}\PY{n}{scores}\PY{p}{)}
    \PY{n}{scores} \PY{o}{=} \PY{n}{scores}\PY{o}{.}\PY{n}{mm}\PY{p}{(}\PY{n}{fc\PYZus{}w}\PY{p}{)} \PY{o}{+} \PY{n}{fc\PYZus{}b}
    \PY{c+c1}{\PYZsh{} *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****}
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
    \PY{c+c1}{\PYZsh{}                                 END OF YOUR CODE                             \PYZsh{}}
    \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
    \PY{k}{return} \PY{n}{scores}
\end{Verbatim}
\end{tcolorbox}

    After defining the forward pass of the ConvNet above, run the following
cell to test your implementation.

When you run this function, scores should have shape (64, 10).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{7}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{three\PYZus{}layer\PYZus{}convnet\PYZus{}test}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{x} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{64}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{32}\PY{p}{,} \PY{l+m+mi}{32}\PY{p}{)}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{dtype}\PY{p}{)}  \PY{c+c1}{\PYZsh{} minibatch size 64, image size [3, 32, 32]}

    \PY{n}{conv\PYZus{}w1} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{dtype}\PY{p}{)}  \PY{c+c1}{\PYZsh{} [out\PYZus{}channel, in\PYZus{}channel, kernel\PYZus{}H, kernel\PYZus{}W]}
    \PY{n}{conv\PYZus{}b1} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{p}{)}\PY{p}{)}  \PY{c+c1}{\PYZsh{} out\PYZus{}channel}
    \PY{n}{conv\PYZus{}w2} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{9}\PY{p}{,} \PY{l+m+mi}{6}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{dtype}\PY{p}{)}  \PY{c+c1}{\PYZsh{} [out\PYZus{}channel, in\PYZus{}channel, kernel\PYZus{}H, kernel\PYZus{}W]}
    \PY{n}{conv\PYZus{}b2} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{9}\PY{p}{,}\PY{p}{)}\PY{p}{)}  \PY{c+c1}{\PYZsh{} out\PYZus{}channel}

    \PY{c+c1}{\PYZsh{} you must calculate the shape of the tensor after two conv layers, before the fully\PYZhy{}connected layer}
    \PY{n}{fc\PYZus{}w} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{9} \PY{o}{*} \PY{l+m+mi}{32} \PY{o}{*} \PY{l+m+mi}{32}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
    \PY{n}{fc\PYZus{}b} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}

    \PY{n}{scores} \PY{o}{=} \PY{n}{three\PYZus{}layer\PYZus{}convnet}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{p}{[}\PY{n}{conv\PYZus{}w1}\PY{p}{,} \PY{n}{conv\PYZus{}b1}\PY{p}{,} \PY{n}{conv\PYZus{}w2}\PY{p}{,} \PY{n}{conv\PYZus{}b2}\PY{p}{,} \PY{n}{fc\PYZus{}w}\PY{p}{,} \PY{n}{fc\PYZus{}b}\PY{p}{]}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{n}{scores}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{p}{)}\PY{p}{)}  \PY{c+c1}{\PYZsh{} you should see [64, 10]}
\PY{n}{three\PYZus{}layer\PYZus{}convnet\PYZus{}test}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([64, 10])
    \end{Verbatim}

    \hypertarget{barebones-pytorch-initialization}{%
\subsubsection{Barebones PyTorch:
Initialization}\label{barebones-pytorch-initialization}}

Let's write a couple utility methods to initialize the weight matrices
for our models.

\begin{itemize}
\tightlist
\item
  \texttt{random\_weight(shape)} initializes a weight tensor with the
  Kaiming normalization method.
\item
  \texttt{zero\_weight(shape)} initializes a weight tensor with all
  zeros. Useful for instantiating bias parameters.
\end{itemize}

The \texttt{random\_weight} function uses the Kaiming normal
initialization method, described in:

He et al, \emph{Delving Deep into Rectifiers: Surpassing Human-Level
Performance on ImageNet Classification}, ICCV 2015,
https://arxiv.org/abs/1502.01852

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{85}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{random\PYZus{}weight}\PY{p}{(}\PY{n}{shape}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Create random Tensors for weights; setting requires\PYZus{}grad=True means that we}
\PY{l+s+sd}{    want to compute gradients for these Tensors during the backward pass.}
\PY{l+s+sd}{    We use Kaiming normalization: sqrt(2 / fan\PYZus{}in)}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{shape}\PY{p}{)} \PY{o}{==} \PY{l+m+mi}{2}\PY{p}{:}  \PY{c+c1}{\PYZsh{} FC weight}
        \PY{n}{fan\PYZus{}in} \PY{o}{=} \PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
    \PY{k}{else}\PY{p}{:}
        \PY{n}{fan\PYZus{}in} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{prod}\PY{p}{(}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{:}\PY{p}{]}\PY{p}{)} \PY{c+c1}{\PYZsh{} conv weight [out\PYZus{}channel, in\PYZus{}channel, kH, kW]}
    \PY{c+c1}{\PYZsh{} randn is standard normal distribution generator. }
    \PY{n}{w} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{n}{shape}\PY{p}{,} \PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{dtype}\PY{p}{)} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{sqrt}\PY{p}{(}\PY{l+m+mf}{2.} \PY{o}{/} \PY{n}{fan\PYZus{}in}\PY{p}{)}
    \PY{n}{w}\PY{o}{.}\PY{n}{requires\PYZus{}grad} \PY{o}{=} \PY{k+kc}{True}
    \PY{k}{return} \PY{n}{w}

\PY{k}{def} \PY{n+nf}{zero\PYZus{}weight}\PY{p}{(}\PY{n}{shape}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{torch}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{shape}\PY{p}{,} \PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{dtype}\PY{p}{,} \PY{n}{requires\PYZus{}grad}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

\PY{c+c1}{\PYZsh{} create a weight of shape [3 x 5]}
\PY{c+c1}{\PYZsh{} you should see the type `torch.cuda.FloatTensor` if you use GPU. }
\PY{c+c1}{\PYZsh{} Otherwise it should be `torch.FloatTensor`}
\PY{n}{random\PYZus{}weight}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{85}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
tensor([[-0.2721, -0.8922,  0.5079,  1.0737, -0.2683],
        [-0.3897,  0.5986, -1.1941,  0.3925, -0.7592],
        [-1.4161, -0.7324, -0.0383, -0.6352, -0.6169]], device='mps:0',
       requires\_grad=True)
\end{Verbatim}
\end{tcolorbox}
        
    \hypertarget{barebones-pytorch-check-accuracy}{%
\subsubsection{Barebones PyTorch: Check
Accuracy}\label{barebones-pytorch-check-accuracy}}

When training the model we will use the following function to check the
accuracy of our model on the training or validation sets.

When checking accuracy we don't need to compute any gradients; as a
result we don't need PyTorch to build a computational graph for us when
we compute scores. To prevent a graph from being built we scope our
computation under a \texttt{torch.no\_grad()} context manager.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{check\PYZus{}accuracy\PYZus{}part2}\PY{p}{(}\PY{n}{loader}\PY{p}{,} \PY{n}{model\PYZus{}fn}\PY{p}{,} \PY{n}{params}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Check the accuracy of a classification model.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Inputs:}
\PY{l+s+sd}{    \PYZhy{} loader: A DataLoader for the data split we want to check}
\PY{l+s+sd}{    \PYZhy{} model\PYZus{}fn: A function that performs the forward pass of the model,}
\PY{l+s+sd}{      with the signature scores = model\PYZus{}fn(x, params)}
\PY{l+s+sd}{    \PYZhy{} params: List of PyTorch Tensors giving parameters of the model}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns: Nothing, but prints the accuracy of the model}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{split} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val}\PY{l+s+s1}{\PYZsq{}} \PY{k}{if} \PY{n}{loader}\PY{o}{.}\PY{n}{dataset}\PY{o}{.}\PY{n}{train} \PY{k}{else} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Checking accuracy on the }\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{ set}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{split}\PY{p}{)}
    \PY{n}{num\PYZus{}correct}\PY{p}{,} \PY{n}{num\PYZus{}samples} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{0}
    \PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)}\PY{p}{:}
        \PY{k}{for} \PY{n}{x}\PY{p}{,} \PY{n}{y} \PY{o+ow}{in} \PY{n}{loader}\PY{p}{:}
            \PY{n}{x} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{dtype}\PY{p}{)}  \PY{c+c1}{\PYZsh{} move to device, e.g. GPU}
            \PY{n}{y} \PY{o}{=} \PY{n}{y}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{int64}\PY{p}{)}
            \PY{n}{scores} \PY{o}{=} \PY{n}{model\PYZus{}fn}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{params}\PY{p}{)}
            \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{preds} \PY{o}{=} \PY{n}{scores}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
            \PY{n}{num\PYZus{}correct} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{n}{preds} \PY{o}{==} \PY{n}{y}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
            \PY{n}{num\PYZus{}samples} \PY{o}{+}\PY{o}{=} \PY{n}{preds}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
        \PY{n}{acc} \PY{o}{=} \PY{n+nb}{float}\PY{p}{(}\PY{n}{num\PYZus{}correct}\PY{p}{)} \PY{o}{/} \PY{n}{num\PYZus{}samples}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Got }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ / }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ correct (}\PY{l+s+si}{\PYZpc{}.2f}\PY{l+s+si}{\PYZpc{}\PYZpc{}}\PY{l+s+s1}{)}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{num\PYZus{}correct}\PY{p}{,} \PY{n}{num\PYZus{}samples}\PY{p}{,} \PY{l+m+mi}{100} \PY{o}{*} \PY{n}{acc}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{barebones-pytorch-training-loop}{%
\subsubsection{BareBones PyTorch: Training
Loop}\label{barebones-pytorch-training-loop}}

We can now set up a basic training loop to train our network. We will
train the model using stochastic gradient descent without momentum. We
will use \texttt{torch.functional.cross\_entropy} to compute the loss;
you can
\href{http://pytorch.org/docs/stable/nn.html\#cross-entropy}{read about
it here}.

The training loop takes as input the neural network function, a list of
initialized parameters (\texttt{{[}w1,\ w2{]}} in our example), and
learning rate.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{train\PYZus{}part2}\PY{p}{(}\PY{n}{model\PYZus{}fn}\PY{p}{,} \PY{n}{params}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Train a model on CIFAR\PYZhy{}10.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Inputs:}
\PY{l+s+sd}{    \PYZhy{} model\PYZus{}fn: A Python function that performs the forward pass of the model.}
\PY{l+s+sd}{      It should have the signature scores = model\PYZus{}fn(x, params) where x is a}
\PY{l+s+sd}{      PyTorch Tensor of image data, params is a list of PyTorch Tensors giving}
\PY{l+s+sd}{      model weights, and scores is a PyTorch Tensor of shape (N, C) giving}
\PY{l+s+sd}{      scores for the elements in x.}
\PY{l+s+sd}{    \PYZhy{} params: List of PyTorch Tensors giving weights for the model}
\PY{l+s+sd}{    \PYZhy{} learning\PYZus{}rate: Python scalar giving the learning rate to use for SGD}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns: Nothing}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{k}{for} \PY{n}{t}\PY{p}{,} \PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{loader\PYZus{}train}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} Move the data to the proper device (GPU or CPU)}
        \PY{n}{x} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{dtype}\PY{p}{)}
        \PY{n}{y} \PY{o}{=} \PY{n}{y}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{long}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Forward pass: compute scores and loss}
        \PY{n}{scores} \PY{o}{=} \PY{n}{model\PYZus{}fn}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{params}\PY{p}{)}
        \PY{n}{loss} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{cross\PYZus{}entropy}\PY{p}{(}\PY{n}{scores}\PY{p}{,} \PY{n}{y}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Backward pass: PyTorch figures out which Tensors in the computational}
        \PY{c+c1}{\PYZsh{} graph has requires\PYZus{}grad=True and uses backpropagation to compute the}
        \PY{c+c1}{\PYZsh{} gradient of the loss with respect to these Tensors, and stores the}
        \PY{c+c1}{\PYZsh{} gradients in the .grad attribute of each Tensor.}
        \PY{n}{loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}

        \PY{c+c1}{\PYZsh{} Update parameters. We don\PYZsq{}t want to backpropagate through the}
        \PY{c+c1}{\PYZsh{} parameter updates, so we scope the updates under a torch.no\PYZus{}grad()}
        \PY{c+c1}{\PYZsh{} context manager to prevent a computational graph from being built.}
        \PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)}\PY{p}{:}
            \PY{k}{for} \PY{n}{w} \PY{o+ow}{in} \PY{n}{params}\PY{p}{:}
                \PY{n}{w} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{learning\PYZus{}rate} \PY{o}{*} \PY{n}{w}\PY{o}{.}\PY{n}{grad}

                \PY{c+c1}{\PYZsh{} Manually zero the gradients after running the backward pass}
                \PY{n}{w}\PY{o}{.}\PY{n}{grad}\PY{o}{.}\PY{n}{zero\PYZus{}}\PY{p}{(}\PY{p}{)}

        \PY{k}{if} \PY{n}{t} \PY{o}{\PYZpc{}} \PY{n}{print\PYZus{}every} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Iteration }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{, loss = }\PY{l+s+si}{\PYZpc{}.4f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{t}\PY{p}{,} \PY{n}{loss}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
            \PY{n}{check\PYZus{}accuracy\PYZus{}part2}\PY{p}{(}\PY{n}{loader\PYZus{}val}\PY{p}{,} \PY{n}{model\PYZus{}fn}\PY{p}{,} \PY{n}{params}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{barebones-pytorch-train-a-two-layer-network}{%
\subsubsection{BareBones PyTorch: Train a Two-Layer
Network}\label{barebones-pytorch-train-a-two-layer-network}}

Now we are ready to run the training loop. We need to explicitly
allocate tensors for the fully connected weights, \texttt{w1} and
\texttt{w2}.

Each minibatch of CIFAR has 64 examples, so the tensor shape is
\texttt{{[}64,\ 3,\ 32,\ 32{]}}.

After flattening, \texttt{x} shape should be
\texttt{{[}64,\ 3\ *\ 32\ *\ 32{]}}. This will be the size of the first
dimension of \texttt{w1}. The second dimension of \texttt{w1} is the
hidden layer size, which will also be the first dimension of
\texttt{w2}.

Finally, the output of the network is a 10-dimensional vector that
represents the probability distribution over 10 classes.

You don't need to tune any hyperparameters but you should see accuracies
above 40\% after training for one epoch.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{hidden\PYZus{}layer\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{4000}
\PY{n}{learning\PYZus{}rate} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}2}

\PY{n}{w1} \PY{o}{=} \PY{n}{random\PYZus{}weight}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{3} \PY{o}{*} \PY{l+m+mi}{32} \PY{o}{*} \PY{l+m+mi}{32}\PY{p}{,} \PY{n}{hidden\PYZus{}layer\PYZus{}size}\PY{p}{)}\PY{p}{)}
\PY{n}{w2} \PY{o}{=} \PY{n}{random\PYZus{}weight}\PY{p}{(}\PY{p}{(}\PY{n}{hidden\PYZus{}layer\PYZus{}size}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}

\PY{n}{train\PYZus{}part2}\PY{p}{(}\PY{n}{two\PYZus{}layer\PYZus{}fc}\PY{p}{,} \PY{p}{[}\PY{n}{w1}\PY{p}{,} \PY{n}{w2}\PY{p}{]}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Iteration 0, loss = 3.3867
Checking accuracy on the val set
Got 130 / 1000 correct (13.00\%)

Iteration 100, loss = 1.9976
Checking accuracy on the val set
Got 336 / 1000 correct (33.60\%)

Iteration 200, loss = 2.0081
Checking accuracy on the val set
Got 336 / 1000 correct (33.60\%)

Iteration 300, loss = 1.8372
Checking accuracy on the val set
Got 398 / 1000 correct (39.80\%)

Iteration 400, loss = 1.7797
Checking accuracy on the val set
Got 401 / 1000 correct (40.10\%)

Iteration 500, loss = 1.8677
Checking accuracy on the val set
Got 436 / 1000 correct (43.60\%)

Iteration 600, loss = 1.8661
Checking accuracy on the val set
Got 418 / 1000 correct (41.80\%)

Iteration 700, loss = 1.8667
Checking accuracy on the val set
Got 429 / 1000 correct (42.90\%)

    \end{Verbatim}

    \hypertarget{barebones-pytorch-training-a-convnet}{%
\subsubsection{BareBones PyTorch: Training a
ConvNet}\label{barebones-pytorch-training-a-convnet}}

In the below you should use the functions defined above to train a
three-layer convolutional network on CIFAR. The network should have the
following architecture:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Convolutional layer (with bias) with 32 5x5 filters, with zero-padding
  of 2
\item
  ReLU
\item
  Convolutional layer (with bias) with 16 3x3 filters, with zero-padding
  of 1
\item
  ReLU
\item
  Fully-connected layer (with bias) to compute scores for 10 classes
\end{enumerate}

You should initialize your weight matrices using the
\texttt{random\_weight} function defined above, and you should
initialize your bias vectors using the \texttt{zero\_weight} function
above.

You don't need to tune any hyperparameters, but if everything works
correctly you should achieve an accuracy above 42\% after one epoch.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{learning\PYZus{}rate} \PY{o}{=} \PY{l+m+mf}{3e\PYZhy{}3}

\PY{n}{channel\PYZus{}1} \PY{o}{=} \PY{l+m+mi}{32}
\PY{n}{channel\PYZus{}2} \PY{o}{=} \PY{l+m+mi}{16}

\PY{n}{conv\PYZus{}w1} \PY{o}{=} \PY{k+kc}{None}
\PY{n}{conv\PYZus{}b1} \PY{o}{=} \PY{k+kc}{None}
\PY{n}{conv\PYZus{}w2} \PY{o}{=} \PY{k+kc}{None}
\PY{n}{conv\PYZus{}b2} \PY{o}{=} \PY{k+kc}{None}
\PY{n}{fc\PYZus{}w} \PY{o}{=} \PY{k+kc}{None}
\PY{n}{fc\PYZus{}b} \PY{o}{=} \PY{k+kc}{None}

\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
\PY{c+c1}{\PYZsh{} TODO: Initialize the parameters of a three\PYZhy{}layer ConvNet.                    \PYZsh{}}
\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
\PY{c+c1}{\PYZsh{} *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****}

\PY{n}{conv\PYZus{}w1} \PY{o}{=} \PY{n}{random\PYZus{}weight}\PY{p}{(}\PY{p}{(}\PY{n}{channel\PYZus{}1}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}
\PY{n}{conv\PYZus{}b1} \PY{o}{=} \PY{n}{zero\PYZus{}weight}\PY{p}{(}\PY{n}{channel\PYZus{}1}\PY{p}{)}
\PY{n}{conv\PYZus{}w2} \PY{o}{=} \PY{n}{random\PYZus{}weight}\PY{p}{(}\PY{p}{(}\PY{n}{channel\PYZus{}2}\PY{p}{,} \PY{n}{channel\PYZus{}1}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}
\PY{n}{conv\PYZus{}b2} \PY{o}{=} \PY{n}{zero\PYZus{}weight}\PY{p}{(}\PY{n}{channel\PYZus{}2}\PY{p}{)}
\PY{n}{fc\PYZus{}w} \PY{o}{=} \PY{n}{random\PYZus{}weight}\PY{p}{(}\PY{p}{(}\PY{n}{channel\PYZus{}2} \PY{o}{*} \PY{l+m+mi}{32} \PY{o}{*} \PY{l+m+mi}{32}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
\PY{n}{fc\PYZus{}b} \PY{o}{=} \PY{n}{zero\PYZus{}weight}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}

\PY{c+c1}{\PYZsh{} *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****}
\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
\PY{c+c1}{\PYZsh{}                                 END OF YOUR CODE                             \PYZsh{}}
\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}

\PY{n}{params} \PY{o}{=} \PY{p}{[}\PY{n}{conv\PYZus{}w1}\PY{p}{,} \PY{n}{conv\PYZus{}b1}\PY{p}{,} \PY{n}{conv\PYZus{}w2}\PY{p}{,} \PY{n}{conv\PYZus{}b2}\PY{p}{,} \PY{n}{fc\PYZus{}w}\PY{p}{,} \PY{n}{fc\PYZus{}b}\PY{p}{]}
\PY{n}{train\PYZus{}part2}\PY{p}{(}\PY{n}{three\PYZus{}layer\PYZus{}convnet}\PY{p}{,} \PY{n}{params}\PY{p}{,} \PY{n}{learning\PYZus{}rate}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Iteration 0, loss = 3.2698
Checking accuracy on the val set
Got 88 / 1000 correct (8.80\%)

Iteration 100, loss = 1.8891
Checking accuracy on the val set
Got 342 / 1000 correct (34.20\%)

Iteration 200, loss = 1.5887
Checking accuracy on the val set
Got 385 / 1000 correct (38.50\%)

Iteration 300, loss = 1.8672
Checking accuracy on the val set
Got 407 / 1000 correct (40.70\%)

Iteration 400, loss = 1.7790
Checking accuracy on the val set
Got 434 / 1000 correct (43.40\%)

Iteration 500, loss = 1.5472
Checking accuracy on the val set
Got 446 / 1000 correct (44.60\%)

Iteration 600, loss = 1.6231
Checking accuracy on the val set
Got 456 / 1000 correct (45.60\%)

Iteration 700, loss = 1.4544
Checking accuracy on the val set
Got 471 / 1000 correct (47.10\%)

    \end{Verbatim}

    \hypertarget{part-iii.-pytorch-module-api}{%
\section{Part III. PyTorch Module
API}\label{part-iii.-pytorch-module-api}}

Barebone PyTorch requires that we track all the parameter tensors by
hand. This is fine for small networks with a few tensors, but it would
be extremely inconvenient and error-prone to track tens or hundreds of
tensors in larger networks.

PyTorch provides the \texttt{nn.Module} API for you to define arbitrary
network architectures, while tracking every learnable parameters for
you. In Part II, we implemented SGD ourselves. PyTorch also provides the
\texttt{torch.optim} package that implements all the common optimizers,
such as RMSProp, Adagrad, and Adam. It even supports approximate
second-order methods like L-BFGS! You can refer to the
\href{http://pytorch.org/docs/master/optim.html}{doc} for the exact
specifications of each optimizer.

To use the Module API, follow the steps below:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Subclass \texttt{nn.Module}. Give your network class an intuitive name
  like \texttt{TwoLayerFC}.
\item
  In the constructor \texttt{\_\_init\_\_()}, define all the layers you
  need as class attributes. Layer objects like \texttt{nn.Linear} and
  \texttt{nn.Conv2d} are themselves \texttt{nn.Module} subclasses and
  contain learnable parameters, so that you don't have to instantiate
  the raw tensors yourself. \texttt{nn.Module} will track these internal
  parameters for you. Refer to the
  \href{http://pytorch.org/docs/master/nn.html}{doc} to learn more about
  the dozens of builtin layers. \textbf{Warning}: don't forget to call
  the \texttt{super().\_\_init\_\_()} first!
\item
  In the \texttt{forward()} method, define the \emph{connectivity} of
  your network. You should use the attributes defined in
  \texttt{\_\_init\_\_} as function calls that take tensor as input and
  output the ``transformed'' tensor. Do \emph{not} create any new layers
  with learnable parameters in \texttt{forward()}! All of them must be
  declared upfront in \texttt{\_\_init\_\_}.
\end{enumerate}

After you define your Module subclass, you can instantiate it as an
object and call it just like the NN forward function in part II.

\hypertarget{module-api-two-layer-network}{%
\subsubsection{Module API: Two-Layer
Network}\label{module-api-two-layer-network}}

Here is a concrete example of a 2-layer fully connected network:

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{TwoLayerFC}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{input\PYZus{}size}\PY{p}{,} \PY{n}{hidden\PYZus{}size}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} assign layer objects to class attributes}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc1} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{input\PYZus{}size}\PY{p}{,} \PY{n}{hidden\PYZus{}size}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} nn.init package contains convenient initialization methods}
        \PY{c+c1}{\PYZsh{} http://pytorch.org/docs/master/nn.html\PYZsh{}torch\PYZhy{}nn\PYZhy{}init }
        \PY{n}{nn}\PY{o}{.}\PY{n}{init}\PY{o}{.}\PY{n}{kaiming\PYZus{}normal\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc1}\PY{o}{.}\PY{n}{weight}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc2} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}size}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{p}{)}
        \PY{n}{nn}\PY{o}{.}\PY{n}{init}\PY{o}{.}\PY{n}{kaiming\PYZus{}normal\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc2}\PY{o}{.}\PY{n}{weight}\PY{p}{)}
    
    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} forward always defines connectivity}
        \PY{n}{x} \PY{o}{=} \PY{n}{flatten}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{n}{scores} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc2}\PY{p}{(}\PY{n}{F}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc1}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}\PY{p}{)}
        \PY{k}{return} \PY{n}{scores}

\PY{k}{def} \PY{n+nf}{test\PYZus{}TwoLayerFC}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{input\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{50}
    \PY{n}{x} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{64}\PY{p}{,} \PY{n}{input\PYZus{}size}\PY{p}{)}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{dtype}\PY{p}{)}  \PY{c+c1}{\PYZsh{} minibatch size 64, feature dimension 50}
    \PY{n}{model} \PY{o}{=} \PY{n}{TwoLayerFC}\PY{p}{(}\PY{n}{input\PYZus{}size}\PY{p}{,} \PY{l+m+mi}{42}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}
    \PY{n}{scores} \PY{o}{=} \PY{n}{model}\PY{p}{(}\PY{n}{x}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{n}{scores}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{p}{)}\PY{p}{)}  \PY{c+c1}{\PYZsh{} you should see [64, 10]}
\PY{n}{test\PYZus{}TwoLayerFC}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([64, 10])
    \end{Verbatim}

    \hypertarget{module-api-three-layer-convnet}{%
\subsubsection{Module API: Three-Layer
ConvNet}\label{module-api-three-layer-convnet}}

It's your turn to implement a 3-layer ConvNet followed by a fully
connected layer. The network architecture should be the same as in Part
II:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Convolutional layer with \texttt{channel\_1} 5x5 filters with
  zero-padding of 2
\item
  ReLU
\item
  Convolutional layer with \texttt{channel\_2} 3x3 filters with
  zero-padding of 1
\item
  ReLU
\item
  Fully-connected layer to \texttt{num\_classes} classes
\end{enumerate}

You should initialize the weight matrices of the model using the Kaiming
normal initialization method.

\textbf{HINT}: http://pytorch.org/docs/stable/nn.html\#conv2d

After you implement the three-layer ConvNet, the
\texttt{test\_ThreeLayerConvNet} function will run your implementation;
it should print \texttt{(64,\ 10)} for the shape of the output scores.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{ThreeLayerConvNet}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{in\PYZus{}channel}\PY{p}{,} \PY{n}{channel\PYZus{}1}\PY{p}{,} \PY{n}{channel\PYZus{}2}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
        \PY{c+c1}{\PYZsh{} TODO: Set up the layers you need for a three\PYZhy{}layer ConvNet with the  \PYZsh{}}
        \PY{c+c1}{\PYZsh{} architecture defined above.                                          \PYZsh{}}
        \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
        \PY{c+c1}{\PYZsh{} *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****}

        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{conv1} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{in\PYZus{}channel}\PY{p}{,} \PY{n}{channel\PYZus{}1}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{conv2} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{channel\PYZus{}1}\PY{p}{,} \PY{n}{channel\PYZus{}2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{channel\PYZus{}2} \PY{o}{*} \PY{l+m+mi}{32} \PY{o}{*} \PY{l+m+mi}{32}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{relu} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}
        \PY{n}{nn}\PY{o}{.}\PY{n}{init}\PY{o}{.}\PY{n}{kaiming\PYZus{}normal\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{conv1}\PY{o}{.}\PY{n}{weight}\PY{p}{)}
        \PY{n}{nn}\PY{o}{.}\PY{n}{init}\PY{o}{.}\PY{n}{kaiming\PYZus{}normal\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{conv2}\PY{o}{.}\PY{n}{weight}\PY{p}{)}
        \PY{n}{nn}\PY{o}{.}\PY{n}{init}\PY{o}{.}\PY{n}{kaiming\PYZus{}normal\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc}\PY{o}{.}\PY{n}{weight}\PY{p}{)}


        \PY{c+c1}{\PYZsh{} *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****}
        \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
        \PY{c+c1}{\PYZsh{}                          END OF YOUR CODE                            \PYZsh{}       }
        \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{n}{scores} \PY{o}{=} \PY{k+kc}{None}
        \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
        \PY{c+c1}{\PYZsh{} TODO: Implement the forward function for a 3\PYZhy{}layer ConvNet. you      \PYZsh{}}
        \PY{c+c1}{\PYZsh{} should use the layers you defined in \PYZus{}\PYZus{}init\PYZus{}\PYZus{} and specify the        \PYZsh{}}
        \PY{c+c1}{\PYZsh{} connectivity of those layers in forward()                            \PYZsh{}}
        \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
        \PY{c+c1}{\PYZsh{} *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****}

        \PY{n}{scores} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{conv1}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{)}
        \PY{n}{scores} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{relu}\PY{p}{(}\PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{conv2}\PY{p}{(}\PY{n}{scores}\PY{p}{)}\PY{p}{)}
        \PY{n}{scores} \PY{o}{=} \PY{n}{flatten}\PY{p}{(}\PY{n}{scores}\PY{p}{)}
        \PY{n}{scores} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{fc}\PY{p}{(}\PY{n}{scores}\PY{p}{)}


        \PY{c+c1}{\PYZsh{} *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****}
        \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
        \PY{c+c1}{\PYZsh{}                             END OF YOUR CODE                         \PYZsh{}}
        \PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
        \PY{k}{return} \PY{n}{scores}


\PY{k}{def} \PY{n+nf}{test\PYZus{}ThreeLayerConvNet}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{n}{x} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{l+m+mi}{64}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{32}\PY{p}{,} \PY{l+m+mi}{32}\PY{p}{)}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{dtype}\PY{p}{)}  \PY{c+c1}{\PYZsh{} minibatch size 64, image size [3, 32, 32]}
    \PY{n}{model} \PY{o}{=} \PY{n}{ThreeLayerConvNet}\PY{p}{(}\PY{n}{in\PYZus{}channel}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{channel\PYZus{}1}\PY{o}{=}\PY{l+m+mi}{12}\PY{p}{,} \PY{n}{channel\PYZus{}2}\PY{o}{=}\PY{l+m+mi}{8}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
    \PY{n}{scores} \PY{o}{=} \PY{n}{model}\PY{p}{(}\PY{n}{x}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{n}{scores}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{p}{)}\PY{p}{)}  \PY{c+c1}{\PYZsh{} you should see [64, 10]}
\PY{n}{test\PYZus{}ThreeLayerConvNet}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
torch.Size([64, 10])
    \end{Verbatim}

    \hypertarget{module-api-check-accuracy}{%
\subsubsection{Module API: Check
Accuracy}\label{module-api-check-accuracy}}

Given the validation or test set, we can check the classification
accuracy of a neural network.

This version is slightly different from the one in part II. You don't
manually pass in the parameters anymore.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{15}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{check\PYZus{}accuracy\PYZus{}part34}\PY{p}{(}\PY{n}{loader}\PY{p}{,} \PY{n}{model}\PY{p}{)}\PY{p}{:}
    \PY{k}{if} \PY{n}{loader}\PY{o}{.}\PY{n}{dataset}\PY{o}{.}\PY{n}{train}\PY{p}{:}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Checking accuracy on validation set}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{k}{else}\PY{p}{:}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Checking accuracy on test set}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}   
    \PY{n}{num\PYZus{}correct} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{n}{num\PYZus{}samples} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{n}{model}\PY{o}{.}\PY{n}{eval}\PY{p}{(}\PY{p}{)}  \PY{c+c1}{\PYZsh{} set model to evaluation mode}
    \PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)}\PY{p}{:}
        \PY{k}{for} \PY{n}{x}\PY{p}{,} \PY{n}{y} \PY{o+ow}{in} \PY{n}{loader}\PY{p}{:}
            \PY{n}{x} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{dtype}\PY{p}{)}  \PY{c+c1}{\PYZsh{} move to device, e.g. GPU}
            \PY{n}{y} \PY{o}{=} \PY{n}{y}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{long}\PY{p}{)}
            \PY{n}{scores} \PY{o}{=} \PY{n}{model}\PY{p}{(}\PY{n}{x}\PY{p}{)}
            \PY{n}{\PYZus{}}\PY{p}{,} \PY{n}{preds} \PY{o}{=} \PY{n}{scores}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}
            \PY{n}{num\PYZus{}correct} \PY{o}{+}\PY{o}{=} \PY{p}{(}\PY{n}{preds} \PY{o}{==} \PY{n}{y}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
            \PY{n}{num\PYZus{}samples} \PY{o}{+}\PY{o}{=} \PY{n}{preds}\PY{o}{.}\PY{n}{size}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
        \PY{n}{acc} \PY{o}{=} \PY{n+nb}{float}\PY{p}{(}\PY{n}{num\PYZus{}correct}\PY{p}{)} \PY{o}{/} \PY{n}{num\PYZus{}samples}
        \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Got }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ / }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{ correct (}\PY{l+s+si}{\PYZpc{}.2f}\PY{l+s+s1}{)}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{num\PYZus{}correct}\PY{p}{,} \PY{n}{num\PYZus{}samples}\PY{p}{,} \PY{l+m+mi}{100} \PY{o}{*} \PY{n}{acc}\PY{p}{)}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{module-api-training-loop}{%
\subsubsection{Module API: Training
Loop}\label{module-api-training-loop}}

We also use a slightly different training loop. Rather than updating the
values of the weights ourselves, we use an Optimizer object from the
\texttt{torch.optim} package, which abstract the notion of an
optimization algorithm and provides implementations of most of the
algorithms commonly used to optimize neural networks.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{16}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{train\PYZus{}part34}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{optimizer}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
\PY{+w}{    }\PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
\PY{l+s+sd}{    Train a model on CIFAR\PYZhy{}10 using the PyTorch Module API.}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Inputs:}
\PY{l+s+sd}{    \PYZhy{} model: A PyTorch Module giving the model to train.}
\PY{l+s+sd}{    \PYZhy{} optimizer: An Optimizer object we will use to train the model}
\PY{l+s+sd}{    \PYZhy{} epochs: (Optional) A Python integer giving the number of epochs to train for}
\PY{l+s+sd}{    }
\PY{l+s+sd}{    Returns: Nothing, but prints model accuracies during training.}
\PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{n}{model} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{)}  \PY{c+c1}{\PYZsh{} move the model parameters to CPU/GPU}
    \PY{k}{for} \PY{n}{e} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{epochs}\PY{p}{)}\PY{p}{:}
        \PY{k}{for} \PY{n}{t}\PY{p}{,} \PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{)} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{loader\PYZus{}train}\PY{p}{)}\PY{p}{:}
            \PY{n}{model}\PY{o}{.}\PY{n}{train}\PY{p}{(}\PY{p}{)}  \PY{c+c1}{\PYZsh{} put model to training mode}
            \PY{n}{x} \PY{o}{=} \PY{n}{x}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{dtype}\PY{p}{)}  \PY{c+c1}{\PYZsh{} move to device, e.g. GPU}
            \PY{n}{y} \PY{o}{=} \PY{n}{y}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{,} \PY{n}{dtype}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{long}\PY{p}{)}

            \PY{n}{scores} \PY{o}{=} \PY{n}{model}\PY{p}{(}\PY{n}{x}\PY{p}{)}
            \PY{n}{loss} \PY{o}{=} \PY{n}{F}\PY{o}{.}\PY{n}{cross\PYZus{}entropy}\PY{p}{(}\PY{n}{scores}\PY{p}{,} \PY{n}{y}\PY{p}{)}

            \PY{c+c1}{\PYZsh{} Zero out all of the gradients for the variables which the optimizer}
            \PY{c+c1}{\PYZsh{} will update.}
            \PY{n}{optimizer}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}

            \PY{c+c1}{\PYZsh{} This is the backwards pass: compute the gradient of the loss with}
            \PY{c+c1}{\PYZsh{} respect to each  parameter of the model.}
            \PY{n}{loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}

            \PY{c+c1}{\PYZsh{} Actually update the parameters of the model using the gradients}
            \PY{c+c1}{\PYZsh{} computed by the backwards pass.}
            \PY{n}{optimizer}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}

            \PY{k}{if} \PY{n}{t} \PY{o}{\PYZpc{}} \PY{n}{print\PYZus{}every} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Iteration }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s1}{, loss = }\PY{l+s+si}{\PYZpc{}.4f}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{t}\PY{p}{,} \PY{n}{loss}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{)}
                \PY{n}{check\PYZus{}accuracy\PYZus{}part34}\PY{p}{(}\PY{n}{loader\PYZus{}val}\PY{p}{,} \PY{n}{model}\PY{p}{)}
                \PY{n+nb}{print}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{module-api-train-a-two-layer-network}{%
\subsubsection{Module API: Train a Two-Layer
Network}\label{module-api-train-a-two-layer-network}}

Now we are ready to run the training loop. In contrast to part II, we
don't explicitly allocate parameter tensors anymore.

Simply pass the input size, hidden layer size, and number of classes
(i.e.~output size) to the constructor of \texttt{TwoLayerFC}.

You also need to define an optimizer that tracks all the learnable
parameters inside \texttt{TwoLayerFC}.

You don't need to tune any hyperparameters, but you should see model
accuracies above 40\% after training for one epoch.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{17}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{hidden\PYZus{}layer\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{4000}
\PY{n}{learning\PYZus{}rate} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}2}
\PY{n}{model} \PY{o}{=} \PY{n}{TwoLayerFC}\PY{p}{(}\PY{l+m+mi}{3} \PY{o}{*} \PY{l+m+mi}{32} \PY{o}{*} \PY{l+m+mi}{32}\PY{p}{,} \PY{n}{hidden\PYZus{}layer\PYZus{}size}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}
\PY{n}{optimizer} \PY{o}{=} \PY{n}{optim}\PY{o}{.}\PY{n}{SGD}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{n}{learning\PYZus{}rate}\PY{p}{)}

\PY{n}{train\PYZus{}part34}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{optimizer}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Iteration 0, loss = 3.3511
Checking accuracy on validation set
Got 131 / 1000 correct (13.10)

Iteration 100, loss = 2.1833
Checking accuracy on validation set
Got 278 / 1000 correct (27.80)

Iteration 200, loss = 1.7219
Checking accuracy on validation set
Got 357 / 1000 correct (35.70)

Iteration 300, loss = 2.1377
Checking accuracy on validation set
Got 406 / 1000 correct (40.60)

Iteration 400, loss = 1.6659
Checking accuracy on validation set
Got 407 / 1000 correct (40.70)

Iteration 500, loss = 1.6724
Checking accuracy on validation set
Got 413 / 1000 correct (41.30)

Iteration 600, loss = 1.8559
Checking accuracy on validation set
Got 450 / 1000 correct (45.00)

Iteration 700, loss = 1.6632
Checking accuracy on validation set
Got 452 / 1000 correct (45.20)

    \end{Verbatim}

    \hypertarget{module-api-train-a-three-layer-convnet}{%
\subsubsection{Module API: Train a Three-Layer
ConvNet}\label{module-api-train-a-three-layer-convnet}}

You should now use the Module API to train a three-layer ConvNet on
CIFAR. This should look very similar to training the two-layer network!
You don't need to tune any hyperparameters, but you should achieve above
above 45\% after training for one epoch.

You should train the model using stochastic gradient descent without
momentum.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{18}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{learning\PYZus{}rate} \PY{o}{=} \PY{l+m+mf}{3e\PYZhy{}3}
\PY{n}{channel\PYZus{}1} \PY{o}{=} \PY{l+m+mi}{32}
\PY{n}{channel\PYZus{}2} \PY{o}{=} \PY{l+m+mi}{16}

\PY{n}{model} \PY{o}{=} \PY{k+kc}{None}
\PY{n}{optimizer} \PY{o}{=} \PY{k+kc}{None}
\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
\PY{c+c1}{\PYZsh{} TODO: Instantiate your ThreeLayerConvNet model and a corresponding optimizer \PYZsh{}}
\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
\PY{c+c1}{\PYZsh{} *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****}

\PY{n}{model} \PY{o}{=} \PY{n}{ThreeLayerConvNet}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{channel\PYZus{}1}\PY{p}{,} \PY{n}{channel\PYZus{}2}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}
\PY{n}{optimizer} \PY{o}{=} \PY{n}{optim}\PY{o}{.}\PY{n}{SGD}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{n}{learning\PYZus{}rate}\PY{p}{)}

\PY{c+c1}{\PYZsh{} *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****}
\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
\PY{c+c1}{\PYZsh{}                                 END OF YOUR CODE                             }
\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}

\PY{n}{train\PYZus{}part34}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{optimizer}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Iteration 0, loss = 3.4938
Checking accuracy on validation set
Got 108 / 1000 correct (10.80)

Iteration 100, loss = 1.7399
Checking accuracy on validation set
Got 360 / 1000 correct (36.00)

Iteration 200, loss = 1.8680
Checking accuracy on validation set
Got 401 / 1000 correct (40.10)

Iteration 300, loss = 1.7497
Checking accuracy on validation set
Got 437 / 1000 correct (43.70)

Iteration 400, loss = 1.7151
Checking accuracy on validation set
Got 431 / 1000 correct (43.10)

Iteration 500, loss = 1.4824
Checking accuracy on validation set
Got 458 / 1000 correct (45.80)

Iteration 600, loss = 1.7442
Checking accuracy on validation set
Got 467 / 1000 correct (46.70)

Iteration 700, loss = 1.6934
Checking accuracy on validation set
Got 453 / 1000 correct (45.30)

    \end{Verbatim}

    \hypertarget{part-iv.-pytorch-sequential-api}{%
\section{Part IV. PyTorch Sequential
API}\label{part-iv.-pytorch-sequential-api}}

Part III introduced the PyTorch Module API, which allows you to define
arbitrary learnable layers and their connectivity.

For simple models like a stack of feed forward layers, you still need to
go through 3 steps: subclass \texttt{nn.Module}, assign layers to class
attributes in \texttt{\_\_init\_\_}, and call each layer one by one in
\texttt{forward()}. Is there a more convenient way?

Fortunately, PyTorch provides a container Module called
\texttt{nn.Sequential}, which merges the above steps into one. It is not
as flexible as \texttt{nn.Module}, because you cannot specify more
complex topology than a feed-forward stack, but it's good enough for
many use cases.

\hypertarget{sequential-api-two-layer-network}{%
\subsubsection{Sequential API: Two-Layer
Network}\label{sequential-api-two-layer-network}}

Let's see how to rewrite our two-layer fully connected network example
with \texttt{nn.Sequential}, and train it using the training loop
defined above.

Again, you don't need to tune any hyperparameters here, but you shoud
achieve above 40\% accuracy after one epoch of training.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{19}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} We need to wrap `flatten` function in a module in order to stack it}
\PY{c+c1}{\PYZsh{} in nn.Sequential}
\PY{k}{class} \PY{n+nc}{Flatten}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{k}{return} \PY{n}{flatten}\PY{p}{(}\PY{n}{x}\PY{p}{)}

\PY{n}{hidden\PYZus{}layer\PYZus{}size} \PY{o}{=} \PY{l+m+mi}{4000}
\PY{n}{learning\PYZus{}rate} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}2}

\PY{n}{model} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
    \PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{,}
    \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{l+m+mi}{3} \PY{o}{*} \PY{l+m+mi}{32} \PY{o}{*} \PY{l+m+mi}{32}\PY{p}{,} \PY{n}{hidden\PYZus{}layer\PYZus{}size}\PY{p}{)}\PY{p}{,}
    \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
    \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{hidden\PYZus{}layer\PYZus{}size}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{,}
\PY{p}{)}

\PY{c+c1}{\PYZsh{} you can use Nesterov momentum in optim.SGD}
\PY{n}{optimizer} \PY{o}{=} \PY{n}{optim}\PY{o}{.}\PY{n}{SGD}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{n}{learning\PYZus{}rate}\PY{p}{,}
                     \PY{n}{momentum}\PY{o}{=}\PY{l+m+mf}{0.9}\PY{p}{,} \PY{n}{nesterov}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}

\PY{n}{train\PYZus{}part34}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{optimizer}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Iteration 0, loss = 2.3229
Checking accuracy on validation set
Got 167 / 1000 correct (16.70)

Iteration 100, loss = 1.6909
Checking accuracy on validation set
Got 375 / 1000 correct (37.50)

Iteration 200, loss = 1.8024
Checking accuracy on validation set
Got 419 / 1000 correct (41.90)

Iteration 300, loss = 1.7323
Checking accuracy on validation set
Got 442 / 1000 correct (44.20)

Iteration 400, loss = 1.7112
Checking accuracy on validation set
Got 426 / 1000 correct (42.60)

Iteration 500, loss = 1.9241
Checking accuracy on validation set
Got 415 / 1000 correct (41.50)

Iteration 600, loss = 1.8353
Checking accuracy on validation set
Got 436 / 1000 correct (43.60)

Iteration 700, loss = 1.6522
Checking accuracy on validation set
Got 453 / 1000 correct (45.30)

    \end{Verbatim}

    \hypertarget{sequential-api-three-layer-convnet}{%
\subsubsection{Sequential API: Three-Layer
ConvNet}\label{sequential-api-three-layer-convnet}}

Here you should use \texttt{nn.Sequential} to define and train a
three-layer ConvNet with the same architecture we used in Part III:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Convolutional layer (with bias) with 32 5x5 filters, with zero-padding
  of 2
\item
  ReLU
\item
  Convolutional layer (with bias) with 16 3x3 filters, with zero-padding
  of 1
\item
  ReLU
\item
  Fully-connected layer (with bias) to compute scores for 10 classes
\end{enumerate}

You should initialize your weight matrices using the
\texttt{random\_weight} function defined above, and you should
initialize your bias vectors using the \texttt{zero\_weight} function
above.

You should optimize your model using stochastic gradient descent with
Nesterov momentum 0.9.

Again, you don't need to tune any hyperparameters but you should see
accuracy above 55\% after one epoch of training.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{84}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{channel\PYZus{}1} \PY{o}{=} \PY{l+m+mi}{32}
\PY{n}{channel\PYZus{}2} \PY{o}{=} \PY{l+m+mi}{16}
\PY{n}{learning\PYZus{}rate} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}2}

\PY{n}{model} \PY{o}{=} \PY{k+kc}{None}
\PY{n}{optimizer} \PY{o}{=} \PY{k+kc}{None}

\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
\PY{c+c1}{\PYZsh{} TODO: Rewrite the 2\PYZhy{}layer ConvNet with bias from Part III with the           \PYZsh{}}
\PY{c+c1}{\PYZsh{} Sequential API.                                                              \PYZsh{}}
\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
\PY{c+c1}{\PYZsh{} *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****}

\PY{n}{conv1} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{channel\PYZus{}1}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
\PY{n}{conv1}\PY{o}{.}\PY{n}{weight} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{random\PYZus{}weight}\PY{p}{(}\PY{p}{(}\PY{n}{channel\PYZus{}1}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n}{conv1}\PY{o}{.}\PY{n}{bias} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{zero\PYZus{}weight}\PY{p}{(}\PY{n}{channel\PYZus{}1}\PY{p}{)}\PY{p}{)}
\PY{n}{conv2} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{channel\PYZus{}1}\PY{p}{,} \PY{n}{channel\PYZus{}2}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
\PY{n}{conv2}\PY{o}{.}\PY{n}{weight} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{random\PYZus{}weight}\PY{p}{(}\PY{p}{(}\PY{n}{channel\PYZus{}2}\PY{p}{,} \PY{n}{channel\PYZus{}1}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n}{conv2}\PY{o}{.}\PY{n}{bias} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{zero\PYZus{}weight}\PY{p}{(}\PY{n}{channel\PYZus{}2}\PY{p}{)}\PY{p}{)}
\PY{n}{fc} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{channel\PYZus{}2} \PY{o}{*} \PY{l+m+mi}{32} \PY{o}{*} \PY{l+m+mi}{32}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
\PY{n}{fc}\PY{o}{.}\PY{n}{weight} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{permute}\PY{p}{(}\PY{n}{random\PYZus{}weight}\PY{p}{(}\PY{p}{(}\PY{n}{channel\PYZus{}2} \PY{o}{*} \PY{l+m+mi}{32} \PY{o}{*} \PY{l+m+mi}{32}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\PY{n}{fc}\PY{o}{.}\PY{n}{bias} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{nn}\PY{o}{.}\PY{n}{Parameter}\PY{p}{(}\PY{n}{zero\PYZus{}weight}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}

\PY{n}{model} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
    \PY{n}{conv1}\PY{p}{,}
    \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
    \PY{n}{conv2}\PY{p}{,}
    \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
    \PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{,}
    \PY{n}{fc}
\PY{p}{)}
\PY{n}{optimizer} \PY{o}{=} \PY{n}{optim}\PY{o}{.}\PY{n}{SGD}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{n}{learning\PYZus{}rate}\PY{p}{,}
                     \PY{n}{momentum}\PY{o}{=}\PY{l+m+mf}{0.9}\PY{p}{,} \PY{n}{nesterov}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{c+c1}{\PYZsh{} print(conv1.weight.shape)}
\PY{c+c1}{\PYZsh{} *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****}
\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
\PY{c+c1}{\PYZsh{}                                 END OF YOUR CODE                             }
\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
\PY{n}{train\PYZus{}part34}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{optimizer}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Iteration 0, loss = 2.4014
Checking accuracy on validation set
Got 125 / 1000 correct (12.50)

Iteration 100, loss = 1.6240
Checking accuracy on validation set
Got 379 / 1000 correct (37.90)

Iteration 200, loss = 1.7898
Checking accuracy on validation set
Got 482 / 1000 correct (48.20)

Iteration 300, loss = 1.7448
Checking accuracy on validation set
Got 470 / 1000 correct (47.00)

Iteration 400, loss = 1.2279
Checking accuracy on validation set
Got 519 / 1000 correct (51.90)

Iteration 500, loss = 1.0525
Checking accuracy on validation set
Got 541 / 1000 correct (54.10)

Iteration 600, loss = 1.1564
Checking accuracy on validation set
Got 539 / 1000 correct (53.90)

Iteration 700, loss = 0.9103
Checking accuracy on validation set
Got 578 / 1000 correct (57.80)

    \end{Verbatim}

    \hypertarget{part-v.-cifar-10-open-ended-challenge}{%
\section{Part V. CIFAR-10 open-ended
challenge}\label{part-v.-cifar-10-open-ended-challenge}}

In this section, you can experiment with whatever ConvNet architecture
you'd like on CIFAR-10.

Now it's your job to experiment with architectures, hyperparameters,
loss functions, and optimizers to train a model that achieves \textbf{at
least 70\%} accuracy on the CIFAR-10 \textbf{validation} set within 10
epochs. You can use the check\_accuracy and train functions from above.
You can use either \texttt{nn.Module} or \texttt{nn.Sequential} API.

Describe what you did at the end of this notebook.

Here are the official API documentation for each component. One note:
what we call in the class ``spatial batch norm'' is called
``BatchNorm2D'' in PyTorch.

\begin{itemize}
\tightlist
\item
  Layers in torch.nn package: http://pytorch.org/docs/stable/nn.html
\item
  Activations:
  http://pytorch.org/docs/stable/nn.html\#non-linear-activations
\item
  Loss functions: http://pytorch.org/docs/stable/nn.html\#loss-functions
\item
  Optimizers: http://pytorch.org/docs/stable/optim.html
\end{itemize}

\hypertarget{things-you-might-try}{%
\subsubsection{Things you might try:}\label{things-you-might-try}}

\begin{itemize}
\tightlist
\item
  \textbf{Filter size}: Above we used 5x5; would smaller filters be more
  efficient?
\item
  \textbf{Number of filters}: Above we used 32 filters. Do more or fewer
  do better?
\item
  \textbf{Pooling vs Strided Convolution}: Do you use max pooling or
  just stride convolutions?
\item
  \textbf{Batch normalization}: Try adding spatial batch normalization
  after convolution layers and vanilla batch normalization after affine
  layers. Do your networks train faster?
\item
  \textbf{Network architecture}: The network above has two layers of
  trainable parameters. Can you do better with a deep network? Good
  architectures to try include:

  \begin{itemize}
  \tightlist
  \item
    {[}conv-relu-pool{]}xN -\textgreater{} {[}affine{]}xM
    -\textgreater{} {[}softmax or SVM{]}
  \item
    {[}conv-relu-conv-relu-pool{]}xN -\textgreater{} {[}affine{]}xM
    -\textgreater{} {[}softmax or SVM{]}
  \item
    {[}batchnorm-relu-conv{]}xN -\textgreater{} {[}affine{]}xM
    -\textgreater{} {[}softmax or SVM{]}
  \end{itemize}
\item
  \textbf{Global Average Pooling}: Instead of flattening and then having
  multiple affine layers, perform convolutions until your image gets
  small (7x7 or so) and then perform an average pooling operation to get
  to a 1x1 image picture (1, 1 , Filter\#), which is then reshaped into
  a (Filter\#) vector. This is used in
  \href{https://arxiv.org/abs/1512.00567}{Google's Inception Network}
  (See Table 1 for their architecture).
\item
  \textbf{Regularization}: Add l2 weight regularization, or perhaps use
  Dropout.
\end{itemize}

\hypertarget{tips-for-training}{%
\subsubsection{Tips for training}\label{tips-for-training}}

For each network architecture that you try, you should tune the learning
rate and other hyperparameters. When doing this there are a couple
important things to keep in mind:

\begin{itemize}
\tightlist
\item
  If the parameters are working well, you should see improvement within
  a few hundred iterations
\item
  Remember the coarse-to-fine approach for hyperparameter tuning: start
  by testing a large range of hyperparameters for just a few training
  iterations to find the combinations of parameters that are working at
  all.
\item
  Once you have found some sets of parameters that seem to work, search
  more finely around these parameters. You may need to train for more
  epochs.
\item
  You should use the validation set for hyperparameter search, and save
  your test set for evaluating your architecture on the best parameters
  as selected by the validation set.
\end{itemize}

\hypertarget{going-above-and-beyond}{%
\subsubsection{Going above and beyond}\label{going-above-and-beyond}}

If you are feeling adventurous there are many other features you can
implement to try and improve your performance. You are \textbf{not
required} to implement any of these, but don't miss the fun if you have
time!

\begin{itemize}
\tightlist
\item
  Alternative optimizers: you can try Adam, Adagrad, RMSprop, etc.
\item
  Alternative activation functions such as leaky ReLU, parametric ReLU,
  ELU, or MaxOut.
\item
  Model ensembles
\item
  Data augmentation
\item
  New Architectures

  \begin{itemize}
  \tightlist
  \item
    \href{https://arxiv.org/abs/1512.03385}{ResNets} where the input
    from the previous layer is added to the output.
  \item
    \href{https://arxiv.org/abs/1608.06993}{DenseNets} where inputs into
    previous layers are concatenated together.
  \item
    \href{https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32}{This
    blog has an in-depth overview}
  \end{itemize}
\end{itemize}

\hypertarget{have-fun-and-happy-training}{%
\subsubsection{Have fun and happy
training!}\label{have-fun-and-happy-training}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{90}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
\PY{c+c1}{\PYZsh{} TODO:                                                                        \PYZsh{}         }
\PY{c+c1}{\PYZsh{} Experiment with any architectures, optimizers, and hyperparameters.          \PYZsh{}}
\PY{c+c1}{\PYZsh{} Achieve AT LEAST 70\PYZpc{} accuracy on the *validation set* within 10 epochs.      \PYZsh{}}
\PY{c+c1}{\PYZsh{}                                                                              \PYZsh{}}
\PY{c+c1}{\PYZsh{} Note that you can use the check\PYZus{}accuracy function to evaluate on either      \PYZsh{}}
\PY{c+c1}{\PYZsh{} the test set or the validation set, by passing either loader\PYZus{}test or         \PYZsh{}}
\PY{c+c1}{\PYZsh{} loader\PYZus{}val as the second argument to check\PYZus{}accuracy. You should not touch    \PYZsh{}}
\PY{c+c1}{\PYZsh{} the test set until you have finished your architecture and  hyperparameter   \PYZsh{}}
\PY{c+c1}{\PYZsh{} tuning, and only run the test set once at the end to report a final value.   \PYZsh{}}
\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
\PY{n}{model} \PY{o}{=} \PY{k+kc}{None}
\PY{n}{optimizer} \PY{o}{=} \PY{k+kc}{None}

\PY{c+c1}{\PYZsh{} *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****}
\PY{n}{channel\PYZus{}1} \PY{o}{=} \PY{l+m+mi}{64}
\PY{n}{channel\PYZus{}2} \PY{o}{=} \PY{l+m+mi}{32}
\PY{n}{channel\PYZus{}3} \PY{o}{=}\PY{l+m+mi}{16}
\PY{n}{model} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
    \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{,} \PY{n}{channel\PYZus{}1}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,}
    \PY{n}{nn}\PY{o}{.}\PY{n}{BatchNorm2d}\PY{p}{(}\PY{n}{channel\PYZus{}1}\PY{p}{)}\PY{p}{,}
    \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
    \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{channel\PYZus{}1}\PY{p}{,} \PY{n}{channel\PYZus{}2}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}
    \PY{n}{nn}\PY{o}{.}\PY{n}{BatchNorm2d}\PY{p}{(}\PY{n}{channel\PYZus{}2}\PY{p}{)}\PY{p}{,}
    \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
    \PY{n}{nn}\PY{o}{.}\PY{n}{Conv2d}\PY{p}{(}\PY{n}{channel\PYZus{}2}\PY{p}{,} \PY{n}{channel\PYZus{}3}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{n}{padding}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,}
    \PY{n}{nn}\PY{o}{.}\PY{n}{BatchNorm2d}\PY{p}{(}\PY{n}{channel\PYZus{}3}\PY{p}{)}\PY{p}{,}
    \PY{n}{nn}\PY{o}{.}\PY{n}{Dropout}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{,}
    \PY{n}{nn}\PY{o}{.}\PY{n}{ReLU}\PY{p}{(}\PY{p}{)}\PY{p}{,}
    \PY{n}{Flatten}\PY{p}{(}\PY{p}{)}\PY{p}{,}
    \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{n}{channel\PYZus{}3}\PY{o}{*}\PY{l+m+mi}{32}\PY{o}{*}\PY{l+m+mi}{32}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{)}
\PY{p}{)}
\PY{n}{optimizer} \PY{o}{=} \PY{n}{optim}\PY{o}{.}\PY{n}{SGD}\PY{p}{(}\PY{n}{model}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}3}\PY{p}{,} \PY{n}{momentum}\PY{o}{=}\PY{l+m+mf}{0.9}\PY{p}{,} \PY{n}{weight\PYZus{}decay}\PY{o}{=}\PY{l+m+mf}{0.001}\PY{p}{)}

\PY{c+c1}{\PYZsh{} *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****}
\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}
\PY{c+c1}{\PYZsh{}                                 END OF YOUR CODE                             }
\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}\PYZsh{}}

\PY{c+c1}{\PYZsh{} You should get at least 70\PYZpc{} accuracy}
\PY{n}{train\PYZus{}part34}\PY{p}{(}\PY{n}{model}\PY{p}{,} \PY{n}{optimizer}\PY{p}{,} \PY{n}{epochs}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Iteration 0, loss = 2.4292
Checking accuracy on validation set
Got 152 / 1000 correct (15.20)

Iteration 100, loss = 1.6838
Checking accuracy on validation set
Got 432 / 1000 correct (43.20)

Iteration 200, loss = 1.3098
Checking accuracy on validation set
Got 484 / 1000 correct (48.40)

Iteration 300, loss = 1.2750
Checking accuracy on validation set
Got 511 / 1000 correct (51.10)

Iteration 400, loss = 1.2416
Checking accuracy on validation set
Got 549 / 1000 correct (54.90)

Iteration 500, loss = 1.1839
Checking accuracy on validation set
Got 523 / 1000 correct (52.30)

Iteration 600, loss = 1.2793
Checking accuracy on validation set
Got 570 / 1000 correct (57.00)

Iteration 700, loss = 1.4143
Checking accuracy on validation set
Got 561 / 1000 correct (56.10)

Iteration 0, loss = 1.1666
Checking accuracy on validation set
Got 568 / 1000 correct (56.80)

Iteration 100, loss = 1.2104
Checking accuracy on validation set
Got 597 / 1000 correct (59.70)

Iteration 200, loss = 1.1542
Checking accuracy on validation set
Got 588 / 1000 correct (58.80)

Iteration 300, loss = 0.9627
Checking accuracy on validation set
Got 618 / 1000 correct (61.80)

Iteration 400, loss = 1.0684
Checking accuracy on validation set
Got 623 / 1000 correct (62.30)

Iteration 500, loss = 0.9707
Checking accuracy on validation set
Got 621 / 1000 correct (62.10)

Iteration 600, loss = 1.0444
Checking accuracy on validation set
Got 626 / 1000 correct (62.60)

Iteration 700, loss = 0.9303
Checking accuracy on validation set
Got 618 / 1000 correct (61.80)

Iteration 0, loss = 1.0095
Checking accuracy on validation set
Got 615 / 1000 correct (61.50)

Iteration 100, loss = 0.7566
Checking accuracy on validation set
Got 643 / 1000 correct (64.30)

Iteration 200, loss = 0.9920
Checking accuracy on validation set
Got 614 / 1000 correct (61.40)

Iteration 300, loss = 1.2189
Checking accuracy on validation set
Got 615 / 1000 correct (61.50)

Iteration 400, loss = 1.0054
Checking accuracy on validation set
Got 611 / 1000 correct (61.10)

Iteration 500, loss = 0.8348
Checking accuracy on validation set
Got 627 / 1000 correct (62.70)

Iteration 600, loss = 0.9101
Checking accuracy on validation set
Got 637 / 1000 correct (63.70)

Iteration 700, loss = 0.8563
Checking accuracy on validation set
Got 640 / 1000 correct (64.00)

Iteration 0, loss = 0.7371
Checking accuracy on validation set
Got 644 / 1000 correct (64.40)

Iteration 100, loss = 0.9677
Checking accuracy on validation set
Got 657 / 1000 correct (65.70)

Iteration 200, loss = 0.9258
Checking accuracy on validation set
Got 665 / 1000 correct (66.50)

Iteration 300, loss = 0.7446
Checking accuracy on validation set
Got 646 / 1000 correct (64.60)

Iteration 400, loss = 0.6923
Checking accuracy on validation set
Got 658 / 1000 correct (65.80)

Iteration 500, loss = 1.0984
Checking accuracy on validation set
Got 664 / 1000 correct (66.40)

Iteration 600, loss = 0.9019
Checking accuracy on validation set
Got 677 / 1000 correct (67.70)

Iteration 700, loss = 1.0565
Checking accuracy on validation set
Got 668 / 1000 correct (66.80)

Iteration 0, loss = 1.0636
Checking accuracy on validation set
Got 660 / 1000 correct (66.00)

Iteration 100, loss = 0.7072
Checking accuracy on validation set
Got 682 / 1000 correct (68.20)

Iteration 200, loss = 0.9449
Checking accuracy on validation set
Got 678 / 1000 correct (67.80)

Iteration 300, loss = 0.6814
Checking accuracy on validation set
Got 650 / 1000 correct (65.00)

Iteration 400, loss = 0.8433
Checking accuracy on validation set
Got 677 / 1000 correct (67.70)

Iteration 500, loss = 1.0455
Checking accuracy on validation set
Got 667 / 1000 correct (66.70)

Iteration 600, loss = 0.8137
Checking accuracy on validation set
Got 676 / 1000 correct (67.60)

Iteration 700, loss = 0.6604
Checking accuracy on validation set
Got 678 / 1000 correct (67.80)

Iteration 0, loss = 0.7686
Checking accuracy on validation set
Got 668 / 1000 correct (66.80)

Iteration 100, loss = 0.8227
Checking accuracy on validation set
Got 682 / 1000 correct (68.20)

Iteration 200, loss = 0.7700
Checking accuracy on validation set
Got 680 / 1000 correct (68.00)

Iteration 300, loss = 0.8455
Checking accuracy on validation set
Got 672 / 1000 correct (67.20)

Iteration 400, loss = 0.6781
Checking accuracy on validation set
Got 670 / 1000 correct (67.00)

Iteration 500, loss = 0.8555
Checking accuracy on validation set
Got 689 / 1000 correct (68.90)

Iteration 600, loss = 1.0529
Checking accuracy on validation set
Got 668 / 1000 correct (66.80)

Iteration 700, loss = 0.9170
Checking accuracy on validation set
Got 679 / 1000 correct (67.90)

Iteration 0, loss = 0.8107
Checking accuracy on validation set
Got 681 / 1000 correct (68.10)

Iteration 100, loss = 0.8105
Checking accuracy on validation set
Got 678 / 1000 correct (67.80)

Iteration 200, loss = 0.7840
Checking accuracy on validation set
Got 684 / 1000 correct (68.40)

Iteration 300, loss = 0.8406
Checking accuracy on validation set
Got 670 / 1000 correct (67.00)

Iteration 400, loss = 0.8474
Checking accuracy on validation set
Got 678 / 1000 correct (67.80)

Iteration 500, loss = 0.6949
Checking accuracy on validation set
Got 683 / 1000 correct (68.30)

Iteration 600, loss = 0.7510
Checking accuracy on validation set
Got 692 / 1000 correct (69.20)

Iteration 700, loss = 0.7945
Checking accuracy on validation set
Got 694 / 1000 correct (69.40)

Iteration 0, loss = 0.8392
Checking accuracy on validation set
Got 696 / 1000 correct (69.60)

Iteration 100, loss = 0.8712
Checking accuracy on validation set
Got 690 / 1000 correct (69.00)

Iteration 200, loss = 0.7057
Checking accuracy on validation set
Got 689 / 1000 correct (68.90)

Iteration 300, loss = 0.9546
Checking accuracy on validation set
Got 694 / 1000 correct (69.40)

Iteration 400, loss = 0.8405
Checking accuracy on validation set
Got 689 / 1000 correct (68.90)

Iteration 500, loss = 0.7791
Checking accuracy on validation set
Got 694 / 1000 correct (69.40)

Iteration 600, loss = 0.6866
Checking accuracy on validation set
Got 695 / 1000 correct (69.50)

Iteration 700, loss = 0.9085
Checking accuracy on validation set
Got 681 / 1000 correct (68.10)

Iteration 0, loss = 0.7502
Checking accuracy on validation set
Got 691 / 1000 correct (69.10)

Iteration 100, loss = 0.5565
Checking accuracy on validation set
Got 696 / 1000 correct (69.60)

Iteration 200, loss = 0.5722
Checking accuracy on validation set
Got 711 / 1000 correct (71.10)

Iteration 300, loss = 0.5252
Checking accuracy on validation set
Got 689 / 1000 correct (68.90)

Iteration 400, loss = 0.6604
Checking accuracy on validation set
Got 705 / 1000 correct (70.50)

Iteration 500, loss = 0.6565
Checking accuracy on validation set
Got 714 / 1000 correct (71.40)

Iteration 600, loss = 0.8018
Checking accuracy on validation set
Got 706 / 1000 correct (70.60)

Iteration 700, loss = 0.6355
Checking accuracy on validation set
Got 719 / 1000 correct (71.90)

Iteration 0, loss = 0.7344
Checking accuracy on validation set
Got 699 / 1000 correct (69.90)

Iteration 100, loss = 0.7316
Checking accuracy on validation set
Got 706 / 1000 correct (70.60)

Iteration 200, loss = 0.7547
Checking accuracy on validation set
Got 695 / 1000 correct (69.50)

Iteration 300, loss = 0.5994
Checking accuracy on validation set
Got 703 / 1000 correct (70.30)

Iteration 400, loss = 0.8575
Checking accuracy on validation set
Got 704 / 1000 correct (70.40)

Iteration 500, loss = 0.6535
Checking accuracy on validation set
Got 713 / 1000 correct (71.30)

Iteration 600, loss = 0.9317
Checking accuracy on validation set
Got 707 / 1000 correct (70.70)

Iteration 700, loss = 0.9961
Checking accuracy on validation set
Got 700 / 1000 correct (70.00)

    \end{Verbatim}

    \hypertarget{describe-what-you-did}{%
\subsection{Describe what you did}\label{describe-what-you-did}}

In the cell below you should write an explanation of what you did, any
additional features that you implemented, and/or any graphs that you
made in the process of training and evaluating your network.

    I modified the threeConvNet above with the following changes: - Added a
new convolution layer - Added BatchNorm2D after each convolution layer -
Added Dropout after the third convolution layer - Changed the number of
filters in each convolution layer from 32, 16 to 64, 32, and 16.

    \hypertarget{test-set-run-this-only-once}{%
\subsection{Test set -- run this only
once}\label{test-set-run-this-only-once}}

Now that we've gotten a result we're happy with, we test our final model
on the test set (which you should store in best\_model). Think about how
this compares to your validation set accuracy.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{91}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{best\PYZus{}model} \PY{o}{=} \PY{n}{model}
\PY{n}{check\PYZus{}accuracy\PYZus{}part34}\PY{p}{(}\PY{n}{loader\PYZus{}test}\PY{p}{,} \PY{n}{best\PYZus{}model}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Checking accuracy on test set
Got 7079 / 10000 correct (70.79)
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]

\end{Verbatim}
\end{tcolorbox}


    % Add a bibliography block to the postdoc
    
    
    
\end{document}

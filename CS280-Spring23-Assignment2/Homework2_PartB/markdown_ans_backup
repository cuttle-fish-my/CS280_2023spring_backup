FullyConnectedNets:

1.
    Sigmoid and ReLU have this problem. For 1-D case, sigmoid function has the following formula:
    $$\delta(x)=\frac{1}{1+e^{-x}}$$
    and its derivative is
    $$\frac{d\delta}{d x} = \frac{e^{-x}}{(1+e^{-x})^2}$$
    The gradient of sigmoid goes to zeros when $|x|$ goes to infinity, which is
    $$\lim_{|x|\to \infty}\frac{e^{-x}}{(1+e^{-x})^2} = 0 $$

    As for ReLU, any input $x$ such that $x<0$ leads to zero gradient since for any $x<0$, ReLU($x$) = 0.

2.
    The five-layer network was more sensitive to the initialization scale. This is because the more parameters there are
    in the network, the more non-convex the loss function becomes. This means that there are more local minima and maxima
    in the loss function, and the gradient can be very sensitive to the initialization scale.

3.
    The reason why AdaGrad tend to make updates small is that the cache is always increasing. Adam does not have this issue,
    because it uses an exponential moving average of the gradient, which will control the cache in a proper interval.

BatchNormalization:

1.
    The results show that batchnorm helps the model to converge faster with higher accuracy and the effect of weight
    initialization is less significant for the model with batchnorm.
    This is because batch normalization has two learnable parameter $gamma$ and $beta$ to scale and shift the normalized
    data, which can be used to compensate the effect of weight initialization.
2.
    The results show that increasing batch size boosts the training accuracy and validation accuracy for the model with
    batchnorm. This is because the larger the batch size is, the more accuarate the mean and variance of the batch are.
    The batch normalization layer can then normalize the data more accurately.
3.
    Option 2 is analogous to layer normalization because it can be seen as a special case of layer normalization when
    gamma = 1 and beta = $\frac{1}{N}$ where $N$ is the number of pixels in one image. At this point, the sum of RGB
    values for all pixels within an image is $\sum (\gamma \hat{x} + \beta) = \sum \hat{x} + \sum \frac{1}{N} = 0 + 1 = 1$.

    Option 3 is analogous to batch normalization because the mean image is shared across all images in the dataset.
    This is similar to the mean of a batch.
4.
    Under option 2 and 3, layer normalization is likely to not work well.
    For option 2, when the dimension of features become very small, then the layer mean and variance will not follow the
    true mean and variance of the whole data.
    For option 3, when the regularization term is high, it constrains the weights of the network, which makes the
    network less flexible. In this case, the learnable parameters gamma and beta are highly limited.
FullyConnectedNets:

1.
    Sigmoid and ReLU have this problem. For 1-D case, sigmoid function has the following formula:
    $$\delta(x)=\frac{1}{1+e^{-x}}$$
    and its derivative is
    $$\frac{d\delta}{d x} = \frac{e^{-x}}{(1+e^{-x})^2}$$
    The gradient of sigmoid goes to zeros when $|x|$ goes to infinity, which is
    $$\lim_{|x|\to \infty}\frac{e^{-x}}{(1+e^{-x})^2} = 0 $$

    As for ReLU, any input $x$ such that $x<0$ leads to zero gradient since for any $x<0$, ReLU($x$) = 0.

2.
    The five-layer network was more sensitive to the initialization scale. This is because the more parameters there are
    in the network, the more non-convex the loss function becomes. This means that there are more local minima and maxima
    in the loss function, and the gradient can be very sensitive to the initialization scale.

3.
    The reason why AdaGrad tend to make updates small is that the cache is always increasing. Adam does not have this issue,
    because it uses an exponential moving average of the gradient, which will control the cache in a proper interval.

BatchNormalization:

1.
    The results show that batchnorm helps the model to converge faster with higher accuracy and the effect of weight
    initialization is less significant for the model with batchnorm.
    This is because batch normalization has two learnable parameter $gamma$ and $beta$ to scale and shift the normalized
    data, which can be used to compensate the effect of weight initialization.
2.
    The results show that increasing batch size boosts the training accuracy and validation accuracy for the model with
    batchnorm. This is because the larger the batch size is, the more accuarate the mean and variance of the batch are.
    The batch normalization layer can then normalize the data more accurately.
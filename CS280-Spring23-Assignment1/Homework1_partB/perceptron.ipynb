{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Learning Algorithm\n",
    "\n",
    "The perceptron is a simple supervised machine learning algorithm and one of the earliest neural network architectures. It was introduced by Rosenblatt in the late 1950s. A perceptron represents a binary linear classifier that maps a set of training examples (of $d$ dimensional input vectors) onto binary output values using a $d-1$ dimensional hyperplane. But Today, we will implement **Multi-Classes Perceptron Learning Algorithm** \n",
    "**Given:**\n",
    "* dataset $\\{(x^i, y^i)\\}$, $i \\in (1, M)$\n",
    "* $x^i$ is $d$ dimension vector, $x^i = (x^i_1, \\dots x^i_d)$\n",
    "* $y^i$ is multi-class target varible $y^i \\in \\{0,1,2\\}$\n",
    "\n",
    "A perceptron is trained using gradient descent. The training algorithm has different steps. In the beginning (step 0) the model parameters are initialized. The other steps (see below) are repeated for a specified number of training iterations or until the parameters have converged.\n",
    "\n",
    "**Step0:** Initial the weight vector and bias with zeros     \n",
    "**Step1:** Compute the linear combination of the input features and weight. $y^i_{pred} = \\arg\\max_k W_k*x^i + b$    \n",
    "**Step2:** Compute the gradients for parameters $W_k$, $b$. **Derive the parameter update equation Here (5 points)**   \n",
    "\n",
    "##################################     \n",
    "TODO: Derive you answer hear\n",
    "#################################\n",
    "$\\Delta W_k = \\left\\{\n",
    "    \\begin{aligned}\n",
    "        & 0, & k = y^i_{pred} = y^i \\\\\n",
    "        & x^i, & k = y^i_{pred} \\neq y^i\n",
    "    \\end{aligned}\n",
    "\\right.$ and $W_k^{new} = W_k^{old} - \\eta\\Delta W_k$\n",
    "\n",
    "$\\Delta b = \\left\\{\n",
    "    \\begin{aligned}\n",
    "        & 0, & k = y^i_{pred} = y^i \\\\\n",
    "        & 1, & k = y^i_{pred} \\neq y^i\n",
    "    \\end{aligned}\n",
    "\\right.$ and $b^{new} = b^{old} - \\eta\\Delta b$\n",
    "\n",
    "\n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "X_Shape: (150, 4)\n",
      "y_Shape: (150,)\n",
      "Label Space: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "print(type(X))\n",
    "y = iris.target\n",
    "y = np.array(y)\n",
    "print('X_Shape:', X.shape)\n",
    "print('y_Shape:', y.shape)\n",
    "print('Label Space:', np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_Shape: (105, 4)\n",
      "X_test_Shape: (45, 4)\n",
      "y_train_Shape: (105,)\n",
      "y_test_Shape: (105,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "## split the training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "print('X_train_Shape:', X_train.shape)\n",
    "print('X_test_Shape:', X_test.shape)\n",
    "print('y_train_Shape:', y_train.shape)\n",
    "print('y_test_Shape:', y_train.shape)\n",
    "\n",
    "print(type(y_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiClsPLA(object):\n",
    "\n",
    "    ## We recommend to absorb the bias into weight.  W = [w, b]\n",
    "\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, lr, num_epoch, weight_dimension, num_cls):\n",
    "        super(MultiClsPLA, self).__init__()\n",
    "        self.X_train = X_train  # N x (D + 1)\n",
    "        self.y_train = y_train  # N x 1\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.weight = self.initial_weight(weight_dimension, num_cls)  # C x (D + 1)\n",
    "        self.sample_mean = np.mean(self.X_train, 0)\n",
    "        self.sample_std = np.std(self.X_train, 0)\n",
    "        self.num_epoch = num_epoch\n",
    "        self.lr = lr\n",
    "        self.total_acc_train = []\n",
    "        self.total_acc_tst = []\n",
    "\n",
    "    def initial_weight(self, weight_dimension, num_cls):\n",
    "        weight = None\n",
    "        #########################################\n",
    "        ##  ToDO: Initialize the weight with   ##\n",
    "        ##  small std and zero mean gaussian   ##\n",
    "        #########################################\n",
    "        weight = np.random.normal(0, 0.01, (num_cls, weight_dimension))\n",
    "\n",
    "        return weight\n",
    "\n",
    "    def data_preprocessing(self, data):\n",
    "        #####################################\n",
    "        ##  ToDO: Normalize the data       ##\n",
    "        #####################################\n",
    "        norm_data = (data - self.sample_mean) / self.sample_std\n",
    "        return norm_data\n",
    "\n",
    "    def train_step(self, X_train, y_train, shuffle_idx):\n",
    "        np.random.shuffle(shuffle_idx)\n",
    "        X_train: np.ndarray = X_train[shuffle_idx]\n",
    "        y_train: np.ndarray = y_train[shuffle_idx]\n",
    "        train_acc = None\n",
    "        ##############################################\n",
    "        ## TODO: to implement the training process  ##\n",
    "        ## and update the weights                   ##\n",
    "        ##############################################\n",
    "        y_pred = np.argmax(X_train @ self.weight.T, axis=0)\n",
    "\n",
    "        train_acc = np.sum(y_pred == y_train) / y_train.size()\n",
    "        self.total_acc_train.append(train_acc)\n",
    "        diff_one_hot = np.eye(X_train.shape[0], self.weight.shape[0])[y != y_pred]  # N x C\n",
    "        self.weight -= self.lr * diff_one_hot.T @ X_train\n",
    "\n",
    "        return train_acc\n",
    "\n",
    "    def test_step(self, X_test, y_test):\n",
    "        X_test = self.data_preprocessing(data=X_test)\n",
    "        num_sample = X_test.shape[0]\n",
    "        test_acc = None\n",
    "\n",
    "        #########################################\n",
    "        ##  ToDO: Evaluate the test set and    ##\n",
    "        ##  return the test acc                ##\n",
    "        #########################################\n",
    "\n",
    "        y_pred = np.argmax(X_test @ self.weight.T, axis=0)\n",
    "        test_acc = np.sum(y_pred == y_test) / y_test.size()\n",
    "        self.total_acc_tst.append(test_acc)\n",
    "\n",
    "        return test_acc\n",
    "\n",
    "    def train(self):\n",
    "        self.X_train = self.data_preprocessing(data=self.X_train)\n",
    "        num_sample = self.X_train.shape[0]\n",
    "\n",
    "        ######################################################\n",
    "        ### TODO: In order to absorb the bias into weights ###\n",
    "        ###  we need to modify the input data.             ###\n",
    "        ###  So You need to transform the input data       ###\n",
    "        ######################################################\n",
    "\n",
    "        self.X_train = np.insert(self.X_train, self.X_train.shape[1], 1)\n",
    "        self.X_test = np.insert(self.X_test, self.X_test.shape[1], 1)\n",
    "\n",
    "        shuffle_index = np.array(range(0, num_sample))\n",
    "        for epoch in range(self.num_epoch):\n",
    "            training_acc = self.train_step(X_train=self.X_train, y_train=self.y_train, shuffle_idx=shuffle_index)\n",
    "            tst_acc = self.test_step(X_test=self.X_test, y_test=self.y_test)\n",
    "            self.total_acc_train.append(training_acc)\n",
    "            self.total_acc_tst.append(tst_acc)\n",
    "            print('epoch:', epoch, 'traing_acc:%.3f' % training_acc, 'tst_acc:%.3f' % tst_acc)\n",
    "\n",
    "    def vis_acc_curve(self):\n",
    "        train_acc = np.array(self.total_acc_train)\n",
    "        tst_acc = np.array(self.total_acc_tst)\n",
    "        plt.plot(train_acc)\n",
    "        plt.plot(tst_acc)\n",
    "        plt.legend(['train_acc', 'tst_acc'])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 5 is different from 105)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-11-269c6a0bec46>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      8\u001B[0m perceptronModel = MultiClsPLA(X_train, y_train, X_test, y_test, lr=1e-3, num_epoch=1000,\n\u001B[1;32m      9\u001B[0m                               weight_dimension=X_train.shape[1] + 1, num_cls=3)\n\u001B[0;32m---> 10\u001B[0;31m \u001B[0mperceptronModel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0mfig\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfigure\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-10-b28ebb3a160b>\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     83\u001B[0m         \u001B[0mshuffle_index\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_sample\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     84\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnum_epoch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 85\u001B[0;31m             \u001B[0mtraining_acc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_train\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mX_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_train\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0my_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mshuffle_idx\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mshuffle_index\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     86\u001B[0m             \u001B[0mtst_acc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtest_step\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_test\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mX_test\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_test\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0my_test\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     87\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtotal_acc_train\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtraining_acc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-10-b28ebb3a160b>\u001B[0m in \u001B[0;36mtrain_step\u001B[0;34m(self, X_train, y_train, shuffle_idx)\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[0;31m## and update the weights                   ##\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m         \u001B[0;31m##############################################\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 45\u001B[0;31m         \u001B[0my_pred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX_train\u001B[0m \u001B[0;34m@\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mT\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     46\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m         \u001B[0mtrain_acc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0my_pred\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0my_train\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m/\u001B[0m \u001B[0my_train\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 5 is different from 105)"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "#######################################################\n",
    "### TODO: \n",
    "### 1. You need to import the model and pass some parameters. \n",
    "### 2. Then training the model with some epoches.\n",
    "### 3. Visualize the training acc and test acc verus epoches\n",
    "perceptronModel = MultiClsPLA(X_train, y_train, X_test, y_test, lr=1e-3, num_epoch=1000,\n",
    "                              weight_dimension=X_train.shape[1] + 1, num_cls=3)\n",
    "perceptronModel.train()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(perceptronModel.total_acc_train)\n",
    "plt.plot(perceptronModel.total_acc_tst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

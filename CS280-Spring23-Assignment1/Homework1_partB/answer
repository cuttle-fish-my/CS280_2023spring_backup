svm.2:
Those pictures look like the "average" pictures of each class. This is because the angle between $w_k$ and $x_i$ affects
the value of $w_k^Tx_i$. Thus, the smaller the angle is, the more similar $w_k$ is to $x_i$ and the higher the probability
that $x_i$ si classified into class $k$.

softmax.1:
Since the weight $W$ is randomly initialized, then the probability $p_i$ should be close to $1/#\{\class\}=0.1$ for each class $i$.
Thus, the value of loss function can be approximated as follows:
$$L=\frac{1}{n}\sum_{i=1}^n -\log p_i = -\log (\prod_{i=1}^n p_i)^\frac{1}{n} \approx -\log ((0.1)^n)^(1/n) = -log 0.1$$

softmax.2:
Given that the loss function of SVM is as follows:
$$L = \frac{1}{n}\sum_{i=1}^n \sum_{j\neq y_i} max(0, 1-(w_j - w_{y_i}})x_i)$$,
if the new added point $x_i$ has the property that $(w_j-w_{y_i})x_i < 1$ for all $j$,
then the loss value will not change

However, for softmax, the loss function takes every training points into consideration. Thus the value will change if we add
a new point $x_i$ into training set.

two_layer_net.1:
I experimented with different hyperparameters combinations, in detail, I choose two different learning_rates: 5e-4 and 1e-3 and two different batch_size: 200 and 400.

The reason why I choose to tune learning_rate is that learning rate is the plot above shows that the loss fluctuated heavily during the training process, then we can increase the learning rate to by increasing 5e-4 each time.
As for batch_size, increase batch size will help the model to get more accurate gradient during training, so we compare the result of original(200) parameter and new (400) parameter.

I traversed through all possible combinations of hyperparameters and save the model with the highest validation accuracy as the best model.

two_layer_net.2:

Actually this problem is equivalent to how to avoid overfitting because overfitting means the model nearly remembered the whole training set. For 1, the more data the model meet, the less likely that it remember all of them. For 2, the more hidden units the model has, the harder for training and the model is less likely to remember all the dataset. For 3, regularization constrains the L2 norm of weights won't be extremely large which means the model won't approximation the implicit function in an extremely complex way.
svm.2:
Those pictures look like the "average" pictures of each class. This is because the angle between $w_k$ and $x_i$ affects
the value of $w_k^Tx_i$. Thus, the smaller the angle is, the more similar $w_k$ is to $x_i$ and the higher the probability
that $x_i$ si classified into class $k$.

softmax.1:
Since the weight $W$ is randomly initialized, then the probability $p_i$ should be close to $1/#\{\class\}=0.1$ for each class $i$.
Thus, the value of loss function can be approximated as follows:
$$L=\frac{1}{n}\sum_{i=1}^n -\log p_i = -\log (\prod_{i=1}^n p_i)^\frac{1}{n} \approx -\log ((0.1)^n)^(1/n) = -log 0.1$$

softmax.2:
Given that the loss function of SVM is as follows:
$$L = \frac{1}{n}\sum_{i=1}^n \sum_{j\neq y_i} max(0, 1-(w_j - w_{y_i}})x_i)$$,
if the new added point $x_i$ has the property that $(w_j-w_{y_i})x_i < 1$ for all $j$,
then the loss value will not change

However, for softmax, the loss function takes every training points into consideration. Thus the value will change if we add
a new point $x_i$ into training set.